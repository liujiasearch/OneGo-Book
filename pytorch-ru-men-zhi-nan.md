---
description: >-
  指南的目的是为了帮助想利用反向传播自己计算神经网络梯度的人，文章里不对算法做过多解释，而是直观地帮助读者知道如何使用该算法，希望读过这篇文章后，读者能自行计算任何形状的神经网络梯度。
---

# 反向传播算法指南

## 规范

神经网络由于结构复杂，如果不规范命名，往往搞不清楚公式中的符号到底指代网络中的哪部分。 一般我们将神经网络画成：

![](.gitbook/assets/sin%20%285%29.svg)

其中，把神经元按层分组，上图从左至右依次是第一层，第二层神经元组。而信号线也是从左至右称作第一组参数，第二组参数，以此类推。 第 n 层的神经元组的输入就是 n 组参数，输出就是n+1 组参数。 本文将给出反向梯度通用算法的说明。

## 正文

大部分的书籍会把单个神经元描述成如下形式：

$$
Y_i=\sigma(\sum{A_ix_i+b_i})
$$

用矩阵来描述由多个这样的神经元组成的一层神经元组是这样子的（一层指竖着的一排）：

$$
Y=\sigma(A*X+B)
$$

神经网络的形状如下图:

![](.gitbook/assets/sin%20%283%29.svg)

以最右边的输出层为例解释上面那个公式。其中 Y 是 2\*1 矩阵，对应神经元组输出， A 是 3\*2 矩阵，对应输入权重， X 是 3\*1 矩阵，表示输入， B是 2\*1 矩阵，表示神经元的偏置。我们如果使用这个的数学模型来考虑问题，面对反向传播时，我们需要把 A\*X 和 B 割裂开来处理，因为他们数学形式是不同的。为了思维的一致性，我们可以调整一下神经网络的结构，使得我们可以使用如下的式子来表示前面的式子：

$$
Y=\sigma(A*X)
$$

如此，涉及到对每一层需要额外放置一个偏置输入，网络图如下：

![](.gitbook/assets/sin%20%287%29.svg)

为了方便理解反向传播， 我们还需要对上面的网络画法再做进一步改进，为了方便，这里去掉了偏置，只考虑输入相关，每层包含 2 个神经元：

![](.gitbook/assets/sin%20%2815%29.svg)

上图把原来直线相连的信号传递线，分步进行了拆分，并做了标记命名。需要注意，我为了节省画图时间，省去了偏置量，但是读者需要心理明白，他们和神经网络的输入是一样的，需要被平等无差别对待。这个神经网络， 有一个输入层，1个隐藏层和1个输出层。其中 记号 $$a$$ 表示神经元的计算输出，特别的与传统记法不同，我把输入输入层的输入 $$X$$ 看作第一个输出 $$a_1$$ （虚线框框起的输入层），即把神经网络的输入层也看作是一个神经网络层，它的神经元是常数变换（\*1），神经导线的权重是1。 $$θ$$ 表示神经导线的权重值。我们反向传播运算的目的就是找到一组 $$θ$$，使得神经网络的损失（误差）函数最小，而且反向传播的目的，就是为了求 $$θ$$ 的梯度。 $$z$$ 是每个神经元的输入，它是前一层 $$a$$ 与神经导线权重 $$\theta$$ 的乘积的和。每层输入乘以权重的值，即神经网络的基本运算 $$A*X$$ 的值。 $$σ$$ 是单个神经元的运算（激活）函数，可以是sigmoid 或者别的什么，我们不关心。由于不讨论正向传播，文章只关心如何计算反向传播，下图是根据反向传播对上图的正向运算方向进行了反转。

![](.gitbook/assets/sin%20%284%29.svg)

为了图示方便，上图省略了一些正向传播时需要计算的量。反向传播是为了求梯度，梯度即导数，因此神经元的计算变成了 $$σ'(z)$$求导。顾名思义，反向传播，我们就要反着来进行计算。一切都要从图示最右边的COST误差函数开始计算。需要特别留意， $$δ$$ 表示误差，是反向运算的核心参数， $$δ_4$$ 数值上等于COST对 $$a_3$$ 求偏导，而后下一层（从右往左看）的$$δ$$ 就是上一层的 $$Δ$$ 经过 $$σ'(z)$$ 运算后得到的。 例如：

$$
δ_3=Δ_4.* σ'(z)
$$

而 $$Δ$$ 则是 $$δ$$ 验证神经导线的发散汇总值。我以隐藏层与输入层之间的网络来解释何谓发散汇总：δ3 在 节点$$\bigodot$$ 处沿着神经导线做 $$θ$$ 计算后发散到不同的神经元上，然后在方形 $$\bigoplus$$ 处进行汇总，这个过程正好和矩阵的乘法运算匹配，可以用 $$Δ^3_{21}=(\theta^2_{32})^T*\delta^3_{31}$$ 来表示。其中上标 $$\delta^3$$ 表示 $$\delta_3$$ ，下标表示 $$n*m$$的矩阵，其中n表示右边的神经元个数，m表示左边的神经元个数，因为反向传播，这么记更自然。要计算出每个 $$\theta$$ 的梯度（导数），最终的运算是 $$\theta'_b=\delta_{b+1}*a_b$$ 。根据这个式子，上图的 $$\theta'_2=\delta_3*a_2$$ 。

说一句题外话，由于神经网络采用批量训练，我们需要一次性求出一个批中的所有导数 $$\theta'$$ 后，再求他们的平均值作为最后返回的梯度值。下面我将模拟演算一次上图的求梯度过程，假设我有100个样本，每个样本输入层有2个输入，中间层有2个，`输`出有3个神经元。 正向传播后我们能得到：样本数m=100，初始化参数 $$\theta^1_{22}，\theta^2_{32}，a^1_{21}，z^2_{21}，a^2_{21}，z^3_{31}，a^3_{31}$$ 。据此计算一遍反向传播：

$$
\theta^4_{31}=\frac{\partial{COST}}{\partial{a_3}}
$$

$$
\Delta^4_{31}=\delta^4_{31}
$$

$$
\delta^3_{31}=\Delta^4_{31}.*\sigma'(z^3_{31})
$$

$$
\theta'^2_{32}=\delta^3_{31}*(a^2_{21})^T
$$

$$
\Delta^3_{21}=(\theta^2_{32})^T*\delta^3_{31}
$$

$$
\delta^2_{21}=\Delta^3_{21}.*\sigma'(z^2_{21})
$$

$$
\theta'^1_{22}=\delta^2_{21}*(a^1_{21})^T
$$

伪代码如下：

```text
For i=1:100
    δ4=ӘCost/Әa3
    Δ4=δ4
    δ3=Δ4.*σ'(z3)
    Θ’2=θ’2+δ3*(a2)^T
    Δ3=(θ2)^T*δ3
    δ2=Δ3.*σ'(z2)
    θ’1=θ’1+δ2*(a1)^T
End
θ’1=(1/m).*θ’1
θ’2=(1/m).*θ’2
```

伪代码需要注意，求 $$\theta$$ 的导数 $$\theta'$$ 时，由于是对一个批次样本进行计算，所以 $$\theta'$$ 是取累加值，一个批量都算完后再求 $$\theta'$$ 的均值。另外地整个步骤中，$$δ_4=\frac{\partial{COST}}{\partial {a}_3}$$ 得根据损失（代价）函数来手工导出式子，一些神经网络框架会提供自动求导，但是知道怎么算的还是有益的，如果损失函数是交叉熵，那么 $$\delta_3$$ 正好等于 $$y-a_3$$ （样本期望-实际网络输出）。

## 进一步讨论

把神经网络简化的看作这么一个函数： $$A(X1,X2)=F(G(X1,T(X2))$$ ，分别对 $$X1$$ 和 $$X2$$ 求偏导得 $$A’(X1)=F’*G’(X1)$$ 和 $$A’(X2)= F’*G’*T’(X2)$$ 。我们在求 $$A$$ 的时候，需要先计算 $$T$$ ，再计算 $$G$$ ，最后计算 $$F$$ 。在求偏导时，我们先求 $$F'$$ ，再求 $$G'$$ ，最后再是 $$T'$$ 。这个和反向传播的理念是一样的。 另外可以发现，要求 $$A’*X2$$，需要求 $$F’*G’$$，这个结果再计算 $$A’*X1$$ 时已经计算过了，因此结合之前的说明，我们可以发现这个 $$F’*G’$$ 其实就是对应算法中的 $$\delta$$ 。而上一层的输出 $$a$$ 则对应 $$T’$$ 作用于 $$X2$$ 上的系数。

## 拓展

在使用梯度下降法时，当神经网络隐藏层特别多的时候很容易发生梯度消失的情况（梯度=0）。为了克服这个问题，有人引入了残差网络，一个残差块的神经元如图：

![Y=X+&#x3C3;\(X\)](.gitbook/assets/sin%20%2811%29%20%282%29.svg)

利用之前提到的算法，我们只需要把 $$X+σ(X)$$ 看作一个新的$$Y=σ(X)$$ 即可，这样网络就又变成了：

![](.gitbook/assets/sin%20%2810%29%20%282%29.svg)

再根据之前的算法，逐步运算即可。

