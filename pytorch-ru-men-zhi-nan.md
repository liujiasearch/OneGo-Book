---
description: >-
  读者可以直接登陆官网参考Pytorch的官方入门指南（https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html），本文可以看作是一个精简后的官方指南中文版。和Keras入门指南类似，这里会跳过安装过程，读者可以自行使用Pip或者参考相关安装指南来完成Pytorch的安装工作。
---

# Pytorch入门指南

## 规范

神经网络由于结构复杂，如果不规范命名，往往搞不清楚公式中的符号到底指代网络中的哪部分。 一般我们将神经网络画成：

![](.gitbook/assets/sin%20%285%29.svg)

其中，把神经元按层分组，上图从左至右依次是第一层，第二层神经元组。而信号线也是从左至右称作第一组参数，第二组参数，以此类推。 第 n 层的神经元组的输入就是 n 组参数，输出就是n+1 组参数。 本文将给出反向梯度通用算法的说明。

## 正文

大部分的书籍会把单个神经元描述成如下形式：

$$
Y_i=\sigma(\sum{A_ix_i+b_i})
$$

用矩阵来描述由多个这样的神经元组成的一层神经元组是这样子的（一层指竖着的一排）：

$$
Y=\sigma(A*X+B)
$$

神经网络的形状如下图:

![](.gitbook/assets/sin%20%283%29.svg)

以最右边的输出层为例解释上面那个公式。其中 Y 是 2\*1 矩阵，对应神经元组输出， A 是 3\*2 矩阵，对应输入权重， X 是 3\*1 矩阵，表示输入， B是 2\*1 矩阵，表示神经元的偏置。我们如果使用这个的数学模型来考虑问题，面对反向传播时，我们需要把 A\*X 和 B 割裂开来处理，因为他们数学形式是不同的。为了思维的一致性，我们可以调整一下神经网络的结构，使得我们可以使用如下的式子来表示前面的式子：

$$
Y=\sigma(A*X)
$$

如此，涉及到对每一层需要额外放置一个偏置输入，网络图如下：

![](.gitbook/assets/sin%20%287%29.svg)

为了方便理解反向传播， 我们还需要对上面的网络画法再做进一步改进，为了方便，我去掉了偏置，只考虑输入相关，每层包含 2 个神经元：

![](.gitbook/assets/sin%20%286%29.svg)

上图把原来直线相连的信号传递线，分步进行了拆分，并做了标记命名。需要注意，我为了节省画图时间，省去了偏置量，但是读者需要心理明白，他们和神经网络的输入是一样的，需要被平等无差别对待。这个神经网络， 有一个输入层，1个隐藏层和1个输出层。其中 记号a 表示神经元的计算输出，特别的与传统记法不同，我把输入输入层的输入X看作第一个输出a1（虚线框框起的输入层），即把神经网络的输入层也看作是一个神经网络层，它的神经元是常数变换（\*1），神经导线的权重是1。θ 表示神经导线的权重值。我们反向传播运算的目的就是找到一组 θ，使得神经网络的损失（误差）函数最小，而且反向传播的目的，就是为了求 θ 的梯度。z 是每个神经元的输入，它是前一层a与神经导线权重 θ 的乘积的和。每层输入乘以权重的值，即神经网络的基本运算 A\*X 的值。 σ 是单个神经元的运算（激活）函数，可以是sigmoid 或者别的什么，我们不关心。由于不讨论正向传播，文章只关心如何计算反向传播，下图是根据反向传播对上图的正向运算方向进行了反转。

![](.gitbook/assets/sin%20%284%29.svg)



