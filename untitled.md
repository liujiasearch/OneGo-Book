---
description: >-
  概要介绍传统棋类的AI算法，主要是alpha-beta剪枝法和蒙特卡洛（monte
  carlo）搜索树。监督学习算法将在专门介绍神经网络的章节里介绍。本章通过井字棋游戏来具体说明算法的实现方法。
---

# 第三章 传统的棋类智能

与其它棋类游戏相比，计算机模拟的围棋人工智能棋力进展相对缓慢。在1997年，就有计算机可以击败世界国际象棋棋王卡斯帕罗夫，但围棋软件直到2016年才有办法击败顶尖围棋棋手。国际象棋目标明确，只要杀死国王即可（中国象棋、日本的将棋状况也差不多），因此算法较为简单。但是围棋以“围”为目标，不一定需要杀死对方棋子，每一步有数百种以上的走法，别切围棋的复杂度高，又极具欺骗性，这些因素都对计算机程序提出了巨大的挑战。虽然像IBM深蓝那样的超级计算机已经能够击败世界上最好的国际象棋棋手，却有不少人能轻易击败围棋软件。黄山谷有诗：“心似蛛丝游碧落，身如蜩甲化枯枝。”，可见围棋给程序员们带来了许多人工智能领域里的挑战，要编写出超越初级水平的计算机围棋程序，是极其困难的一回事。

## 3.1 极小化极大算法

极小化极大算法常用于棋类等由两方较量的游戏和程序，它可以追溯到中世纪，是一种找出失败的最大可能性中的最小值的算法。算法的基本思想是假设游戏总是零总和的，即双方都是在可选的选项中选择将自己的优势最大化的选择，而另一方总是选择令对手优势最小化的方法。很多棋类游戏可以采取此算法，例如井字棋（tic-tac-toe）。

极小化极大算法顾名思义，就是让最大的情况最小，这里的最大一般是指最差的情况，比如游戏中最不利的情况，再通俗一点就是说下棋时己方要时刻保证最小化对手方的最大收益。 算法的前提是游戏需要满足零和博弈，两个玩家进行游戏，如果其中一方得到利益那么另一方就会失去利益，游戏利益的总和为0或者某一常数，如果不满足这个条件，算法就将失去意义。本质上极小化极大算法就是一个树形结构的递归算法，每个节点的子节点和父节点都是对方玩家，所有的节点被分为两类，分别是我方的极大值节点和对方的极小值节点。

下面以井字棋小游戏为例，我们来实现一个会玩井字棋的AI小程序。为了解释和后续编程的方便，我们约定井字棋的执X方先落子。图3-1，轮到执X方，显然此时下到右下角就能赢取胜利。我们需要为AI定义一个知道怎么在下一步获胜的函数。

![&#x56FE; 3 - 1 &#x6267;X&#x65B9;&#x8BE5;&#x8D70;&#x54EA;&#x4E00;&#x6B65;&#xFF1F;](.gitbook/assets/image%20%2820%29.png)

{% code title="myGO\\tic-tac-toe\\agent.py" %}
```python
def findWinMove(game_state,player):
	possible_moves = []		
	for candidate_move in game_state.legal_moves(player):			#1
		next_game_state = game_state.apply_move(candidate_move)		#2
		if next_game_state.is_over() and next_game_state.winner == player:
			possible_moves.append(candidate_move)					#3
	return possible_moves											#4
```
{% endcode %}

1. 遍历当前玩家（player）所有合法选择（move）；
2. 得到每个选择带来的结果；
3. 如果当前选择能带来胜利，就保存起来；
4. 遍历完后返回保存的落子位。

通过以上算法，就能轻易看出一个弊端，仅仅是要想知道哪一步一定能取胜，就需要遍历所有可能的选择。这在象棋这种有固定棋子和固定下棋方式的游戏中或许可行，但是在围棋中实在是太困难了，因为算法所带来的计算量将是灾难性的。

让我们想想，双方是怎么会下到这个局面的呢？让我们把游戏倒退一步，回到图3-2的样子。执X方选择了左上角和中心的位置，执O方现在占据了左下角，现在轮到他走一步，他会很天真地想，如果自己占据了下路的中间位置，就能在下下一步中连城一线吗？这似乎是很可笑的，因为执X方会比他先连城一线。显然我们的AI必须要知道不要给对方获胜的机会，具体到井字棋游戏，也就是要占据对方能够获胜的位置。转换到程序语言，如果己方这一步不能获胜，就不要让对方的下一步能获胜。现在我们就需要定义一个找到避免自己失败行为的函数。

![&#x56FE; 3 - 2 &#x6267;O&#x65B9;&#x8BE5;&#x8D70;&#x54EA;&#x4E00;&#x6B65;&#xFF1F;](.gitbook/assets/image%20%283%29.png)

{% code title="myGO\\tic-tac-toe\\agent.py" %}
```python
def findMinLoseMoves(game_state,player):
	opponent = player.other()										#1
	possible_moves = []
	for candidate_move in game_state.legal_moves(player):			#2 
		next_state = game_state.apply_move(candidate_move)			#3
		opponent_winning_move = findWinMove(next_state, opponent)	#4
		if opponent_winning_move is None:
			possible_moves.append(candidate_move)					#5
	return possible_moves
```
{% endcode %}

1. 初始化己方的对手；
2. 遍历自己所有的合法选择；
3. 计算执行该选择后留给对方的新局面；
4. 对方能否在这个新局面获胜；
5. 如果不能，加入己方可选项的集合内。

显然AI在调用findMinLoseMoves\(\)这个函数前需要先调用findWinMove\(\)，只有知道自己不会赢，再去思考防止对方胜利的选择。由于只要胜利了，游戏就结束了。如果己方能够找到防止对方胜利的选择，相反，对方也会使用相同的策略，所以无论一方如何选择，对方总有应对之策。按这个逻辑，在零和游戏中，单纯模拟寻找胜利的选择是没有意义的，因为算法总是会在对方选择必胜的行动前就已经扼杀了这个选项。

如果双发在当前回合都无法必胜，我们就要继续搜寻下去，看看是不是存在对方至少要连下两步棋才可以阻止己方的胜利。图3-3，如果执X方落棋到中心红色X点，那么执O方必须同时堵住两个红色O的位置才可以避免执X方的胜利。根据这个想法，我们再实现一个函数，在调用findMinLoseMoves\(\)之后，对它返回的可选项进行筛选。

![&#x56FE; 3 - 3 &#x6267;X&#x65B9;&#x5FC5;&#x80DC;&#x7684;&#x7B56;&#x7565;&#x662F;&#x54EA;&#x4E00;&#x6B65;&#xFF1F;](.gitbook/assets/image%20%2815%29.png)

{% code title="myGO\\tic-tac-toe\\agent.py" %}
```python
def findNextWinMove(game_state, player):
	opponent = player.other()
	for candidate_move in game_state.legal_moves(player): 
		next_state = game_state.apply_move(candidate_move) 
		good_responses = findMinLoseMoves(next_state, opponent)		#1
		if not good_responses:
			return candidate_move
	return None
```
{% endcode %}

1. 这一步是整个函数的关键，逻辑上却很简单。如果己方当前选择使得对方在下一步无法阻止己方在下下一步获胜，那么当前一步棋必然是制胜棋。

如果一方AI能找到一步必胜棋，逻辑上对方应该已经在上一步行棋时占据了这个位置，或者提前扼杀了这种可能性。所以如果可以穷举，依照算法的逻辑，任何一局棋从一开始就已经注定了谁胜谁负。不仅是井字棋，几乎任何棋类，只要先下的一方不出现失误，这一局就起码是和局，也就是在一开始提到的零和。

再回顾一下上面这个算法中AI考虑的完整思路：

* 当前步能不能立即胜利，如果能就执行，否则就进入下一步；
* 能不能阻止对方在下一步胜利，如果不能就认输，否则进入下一步；
* 能不能找到自己在下一步中立即胜利，如果能就执行，否则进入下一步；
* 查看对方是不是在下一步存在必胜的下法，如果不能就认输，否则继续进入下一步；

              。。。。。。

* 能不能找到自己在下一步中必胜的下法，如果能就执行，否则进入n；
* 查看对方是不是在下一步存在必胜的下法，并且能否阻止，否则继续进入下一步；

             。。。。。。

从上述的算法我们可以看出，整个算法即简单又粗暴，计算过程试图穷尽所有可能的选择，如果己方下一步不能胜利，就尝试阻止对方在下一步胜利，之后再考虑己方在下下一步能否胜利，如果不能则尝试阻止对方在下下一步的胜利，以这种逻辑不断地模拟行棋的过程，只要己方在落子时不能胜利，就将子落在能阻止对方胜利的位置，直到达到棋局终止的条件。即便是像井字棋这种非常简单，仅有9个落子位的棋类游戏，第一步行棋要搜索的空间也达到94981步，虽然对现代计算机而言这算不上是很深的深度，但如果是象棋，数字就会变得非常可怕，以现代计算机的处理能力，这种贪婪算法所耗费的时间是不可接受的（计算一步棋大概要算上好几年）。假使换做是围棋，这个算法就会更加耗时冗长，几乎是会永无止境的运算下去。

虽然算法不太理想，但是对于井字棋，它已经足够好了。最后为了完成这个AI，我们还需要使用一个小技巧，因为如果简单地照抄上面的算法，我们必须手工编写每一步判断函数。当然如果我们只考虑未来的有限步，这种方式是可行的。由于大部分棋类使用极小化极大算法将耗费大量的时间，所以可行的一种妥协方式就是模拟未来的有限步，并从结果中取出已知的最优选项。不过由于井字棋的规模比较小，我们无需考虑这些，但是在介绍这个小技巧前，我们先定义几个游戏的枚举类，这和在上一章所做的工作类似，定义枚举类仅仅是为了使得代码更加可读，也为了编程时方便对变量记忆。

{% code title="myGO\\tic-tac-toe\\main.py" %}
```python
class GameResult(enum.Enum):    #1
    loss = 1
    draw = 2
    win = 3
class GameState(enum.Enum):     #2
    waiting=0
    running=1
    over=2
    
player_x=1                      #3
player_o=-1
```
{% endcode %}

1. 井字棋的结局存在3种，这和大部分游戏一致。为了后面对棋局优劣的判断方便，对枚举值的赋值按输到赢，数值安排由小到大，这样安排的便利性将在后面实际编程中体现；
2. 定义游戏的状态，主要是为了方便AI知道游戏什么时候结束；
3. 为了编程上的方便，我没有为执棋的双方再定义一个枚举类，而是直接为其赋了变量值，这在实践上并不是一个好的习惯，但是目前在我们的演示中这不会是一个问题。

接着和大部分棋类一样，我们把下棋这件事拆分成3个主要的对象类。棋盘和下棋的人是显而易见的实体，而棋局的状态属于一个比较抽象的概念，简单来说，将它实例化后就可以代表一局棋，现实生活中，当我们有了棋盘，有了下棋的人，那么剩下的事情就是下棋了，我们下第一局，第二局，第三局等等直到厌烦，而这每一个具体的某一局棋就看作是棋局状态类的一个实例。我们还可以额外定义一个裁判类，不过由于井字棋规则简单，这里就不再额外定义这个类，而是把裁判的功能放在棋局的状态类里实现。

先来看一下抽象后的棋盘：

{% code title="myGO\\tic-tac-toe\\ttt.py" %}
```python
class Board:
    def __init__(self):                #1
        self.board=np.zeros((3,3))
    def printBoard(self):              #2
        for i in range(3):
            line='|'
            ele=[]
            for j in range(3):
                if self.board[i,j]==-1:
                    ele.append('_O_')
                elif self.board[i,j]==1:
                    ele.append('_X_')
                else:
                    ele.append('___')
            print(line.join(ele))      #3
```
{% endcode %}

1. 将井字棋的棋盘初始化为一个3×3的全零numpy数组，X用1表示，O用-1表示；
2. 定义一个打印棋盘的方法，方便人机交互；
3. 按行来打印棋盘。

对象编程有个特点，就是每个人对一件事情都有各自不同的认识与抽象结果。这里我把Board类抽象的很简单，就是一个棋盘，我们能做的就是去看它（通过打印方法），这也与实际生活相符，如果你愿意，也可以给它加上落子的方法，从而更新`Board.board`的值，但是我打算把更新`Board.board`这件事交给别的类来做。

和智能算法无关的事情，我打算都交给记录棋局的状态类来完成，来看看它需要做些什么：

{% code title="myGO\\tic-tac-toe\\ttt.py" %}
```python
class Game:
    def __init__(self, board, player=player_x,game_state=GameState.waiting):
        self.board=board                                        #1
        self.player=player                                      #2
        self.winner=None
        self.state=game_state
        self.bot1=None
        self.bot2=None

    def getResult(self):                                        #3                   
        
    def run(self,mode='hvh',bot1_mode='r',bot2_mode='r'):       #4
        
    def applyMove(self,move):                                   #5
        
    def simuApplyMove(self,move):                               #6
        
    def isLegalMove(self,move):                                 #7
        
    def getLegalMoves(self):                                    #8
        
```
{% endcode %}

1. 将棋局和棋盘挂钩，表示某一局棋下在哪副棋盘上；
2. 记录当前回合属于哪一方；
3. 判断当前棋局状态，这个方法将返回之前定义的枚举类`GameState`的值；
4. 用`run`这个方法来启动游戏，执棋双方在这个方法中轮流下棋，其中mode用来提示下棋模式，游戏可以支持人与人的对战，人与AI的对战以及AI和AI之间的对战；
5. `applyMove`用来更新棋盘，也可将这个方法放在Board类中；
6. `simuApplyMove`做的事情和`applyMove`类似，只不过一个是真的在棋盘上落子，一个只是模拟AI在思考时虚拟落子。所以`simuApplyMove`这个方法会额外返回一个Game实例，因为我们不能在原棋盘上更新思考时假想的落子，这一点和人类在下棋时的行为是类似的；
7. 判断落子是否合法，井字棋游戏不允许在已经下过的位置再下棋，一般也可以为这种游戏规则的校验再额外设置一个裁判类，使得逻辑抽象和分工更加明确；
8. 这个类返回当前棋局状态下所有合法的选择，这个方法也可以放在Agent类中实现，但是为了保持风格的一致，我选择了尽量简化另外两个类。

上面这些方法都和本章的智能主题不太相关，所以具体的实现可以访问myGO在github上的源码，源码里有更详细的注解，这里就不再累述。

和棋盘类一样，我不想把下棋的人这件事情抽象的过于复杂，AI只需要能根据棋盘的当前状态给出下一步出棋就行了。

{% code title="myGO\\tic-tac-toe\\ttt.py" %}
```python
class Agent:
    def __init__(self,game,player,mode='r'):     
        self.game=game                           #1
        self.player=player                       #2
        self.mode=mode                           #3    
    def chooseMove(self):                        #4
        if self.mode=='r':                       
            moves=self.game.getLegalMoves()
            return random.choice(moves)            
        if self.mode=='ai':                      
```
{% endcode %}

1. 把Ai和具体的游戏实例挂钩，相当于告诉AI，它在下哪盘棋；
2. 给AI分配角色，告诉它是执X方还是执O方；
3. AI可以有很多不同的智能算法，我们用mode来告诉它应该使用哪种算法；
4. 定义出棋的方法，这里我们先提供两种方法，随机方法和贪婪的极小化极大算法。

随机方法的棋力非常弱，它从所有合法的选项中随机挑选出一个选择，专门实现随机方法这件事的目的是为了给后面的极小化极大算法提供一个参考对手，我们后面会看到贪婪算法与随机算法在棋力方面的差距。





## 3.2 Alpha-beta剪枝算法

Alpha-beta剪枝是一种搜索算法，其出现的目的是为了减少极小化极大算法（Minimax算法）搜索树的节点数。1997年5月11日，击败加里·卡斯帕罗夫的IBM”深蓝“就采用了这种算法。



## 3.3 蒙特卡洛方法

### 3.3.1蒙特卡洛算法

蒙特卡洛方法是科学家冯·诺伊曼、斯塔尼斯拉夫·乌拉姆和尼古拉斯·梅特罗波利斯在洛斯阿拉莫斯国家实验室为核武器计划工作时发明的一种以概率统计理论为指导的数值计算方法。它是一种使用随机数来解决计算问题的统计模拟方法。

很多实际的问题中，我们无法得到精确的数值解，工程中往往可以接受在误差允许范围内的结果。例如虽然我们知道圆的面积公式是：

$$
S=\pi*r^2
$$

但是π是一个无理数，在处理实际问题时我们总是根据实际情况，截取小数点后满足要求的位数。比如劳动人民小明想要通过共享经济出租自己房屋的一个圆形淋浴房，淋浴房的半径是45厘米，共享App上规定出租价格必须按淋浴房的实际面积来算，小明的房屋位置靠近CBD区，可以按一平方米20元的价格计算每次的出租价格，他能以什么价格出租呢？

{% hint style="info" %}
根据圆形的面积公式，小明的淋浴室占地0.6361725123519332平方米，按App里的约定，可以每次以12.723450247038663元出租。人民币的最小单位是分，四舍五入，小明每次出租后可以拿到12.72元。以这个价格反向推算出小明的淋浴室面积是0.636平方米，和理论值的误差接近0.027%。
{% endhint %}

如果圆的面积公式从未被发现，将淋浴房设计成圆形的小明是否就无法出租自己的淋浴房呢？非常幸运，蒙特卡洛方法可以帮助小明。通过设计某种统计模拟，就可以近似地计算圆的面积。如果计算得到的精度误差也在0.027%左右，那么在实际的日常生活中就可以用这种模拟来作为圆面积的计算方法。

半径为0.45米的圆一定外切一个边长为0.9米的正方形。我们设计一个随机数生成器，它的作用就是随机输出这个边长为0.9米的正方形中点的位置。我们可以用圆中包含点的数量占正方形中点的总数的比例来近似评估圆形面积在正方形面积中的占比。

![](.gitbook/assets/wei-ming-ming-hui-tu-9%20%281%29.svg)

根据算法，我们设计了下面这个小工具，建议读者自行调整其中随机数产生的次数，以观察随机次数对最终结果的影响。

{% code title="myGO\\tic-tac-toe\\cycle\_area.py" %}
```python
import numpy as np
from numpy import linalg as LA
r=0.45 #半径
side_len=2*r #正方形边长
square_area=side_len**2 #正方形面积
class area_machine:
    def __init__(self,times,r):
        self.times=times #随机生成次数
        self.len=r #圆形的半径
    def isInsideCycle(self,gen):
        return LA.norm(gen,axis=1)<self.len
    def random_generator(self,times):
        return (-1+2*np.random.rand(times,2))*self.len #在正方形中随机生成点
    def run(self):
        dots=self.random_generator(self.times)
        dots_in_cycle=self.isInsideCycle(dots) #查找在圆内的点
        dots_in_cycle_n=np.sum(dots_in_cycle)
        return (dots_in_cycle_n/self.times)*square_area #返回圆的面积
        
times=[100,1000,10000,100000,1000000,10000000] #请尝试修改这里的数字
np.random.seed(2020)
print([area_machine(i,r).run() for i in times])          
```
{% endcode %}

通过小工具的计算，可以得到半径为0.45米的圆面积为：

| 随机次数 | 100 | 1000 | 10000 | 100000 | 1000000 | 10000000 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 圆的面积 | 0.5994 | 0.63828 | 0.639171 | 0.6363117 | 0.63647127 | 0.636210045 |

当随机10000次后，我们得到的面积就已经可以在实际应用中使用了。从结果中也可以看出，随着随机次数的增加，我们得到的结果和实际值越接近（0.6361725123519332）.但是也可以看到，随机一百万次得到的结果反而精度没有随机十万次高，这主要是由于电脑产生的随机数方法属于伪随机，并不是真正的随机。但是只要随机次数足够多（比如一千万次），总是可以得到令人满意的精度。在一些精度要求不高的应用场景下，随机一千次以后得到的值就已经比较准确了（误差0.33%）。我们也观察到，随着采样次数的增加，蒙特卡洛方法需要更多的时间用于生成模拟数据。如果想要节约时间，可行的一种方案是采用并行计算。但是数值精度和计算资源消耗之间总是会构成一对矛盾。蒙特卡洛方法的优点是简单易用，只需正确地构造概率过程，就可以建立目标估计量，但是对于高精度的要求会导致计算资源消耗线性增加，因此这也成为该方法一个为人诟病的缺点。

### 3.3.2蒙特卡洛搜索树

1987年，布鲁斯·艾布拉姆森在他的博士论文中率先探索了蒙特卡洛方法在棋类中的运用。1992年，B·布鲁格曼首次将其应用于对弈智能程序，但当时并没有受到重视。2006年， 雷米·库洛姆在其2007年的一篇论文中描述了蒙特卡洛方法在游戏树搜索的应用，并将其命名为蒙特卡洛树搜索。之后列文特·科奇什和乔鲍·塞派什瓦里将蒙特卡洛树搜索方法与UCB公式结合，开发了UCT算法。MoGo、Fuego等软件基于UCT算法，可以在九路围棋中击败了人类业余棋手。

下棋时，棋手需要根据当前局面情况选择对自己最有利的落子点，人类选手会采用类似于Alpha-beta算法的思维策略，蒙特卡洛搜索树方法则采用另外一种策略，通过随机的模拟双方落子，最后汇总找出胜率最高的落子位。通常如果模拟的次数达到一定数量，算法总可以返回一个差不多理想的结果，如果模拟的次数足够多，算法结果会非常逼近实际的最优值。本质上，蒙特卡洛搜索树通过多次实施弱算法，用类似投票的制度对结果进行评价，从而形成一个强效的算法。

![](.gitbook/assets/svg1%20%282%29.svg)

以井字棋为例，当执X方落子后，执O方有8个选择，单纯的蒙特卡洛算法不对局面进行评判，为了确认哪个选择更有利，算法随机选择一处O方落子后开始仿真双方落子，直到游戏结束，这样就完成了一次随机采样过程。每一步落子都是随机的，算法每随机仿真一步就会产生一个子树枝节点，每仿真完一局棋局，就会在根节点上产生一条完整的树干。模拟的棋局越多，树枝就会越繁茂。

仿真完一局后，算法需要记录棋局结果，为了使每次仿真的数据能够被最大化利用，结果不仅是保留在当前可选的树枝节点上，所有被仿真到的节点都应该记录本节点此次仿真的胜负结果。一种简单的策略是从最末的节点开始更新，顺序更新搜索路径上对应的父节点，反向传播直到当前节点为止。更新时，可以采用赢一局计一分，输一局减一分，平局不得分的简单记法。

![](.gitbook/assets/wei-ming-ming-hui-tu-10.svg)

当仿真了足够多的棋局后，蒙特卡洛算法总是选择得分最高的节点作为最优节点的估计。图示中，蓝色节点代表游戏对手执X方实际的落子点，红色节点代表执O的己方根据蒙特卡洛算法选择的落子点。己方作为后手一开始面对的是对手落子X0后的局面，根据算法仿真的结果，己方选择O1点作为自己的落子，此时己方可以先维持住当前的搜索树，直到对手下出X1的局面。当对手落子X1后，与O1节点并列的其它兄弟节点就失去了价值，因为他们所代表的落子顺序已不会再在棋局中出现。算法可以将X1局面及其下面的枝叶剪出，作为一个全新的搜索树来对待，这样己方之前在这个局面下做过的计算可以保留。如果计算能力足够，也可以每个局面都作为一个全新的搜索树来对待，这样做的好处是程序易于实现，但是抛弃了历史的仿真记录，计算资源的浪费比较严重。有些时候，由于实际环境的限制，模拟的次数可能无法覆盖全部可选项。比如对方下出X2的局面，这个情况之前并没有模拟到，算法可以选择将其看作一个全新的节点，剪切出去后，抛弃所有的历史数据。

归结起来，算法做了这么几件事情：

{% code title="myGO\\tic-tac-toe\\ttt.py" %}
```python
class Agent:
    
    ...
        
    def chooseMove(self): 
        
        ...

        if self.mode=='mt': 
        		try_times=1000 #1        		
        		if self.game.last_move is not None:	#2	
        				node_tmp=self.tree.findNextNodeByMove(self.tree.tree_root, \
                    self.game.last_move) #3
        				if node_tmp==None: #4
        						self.tree.node_name+=1
        						self.tree.tree_root=Node(str(self.tree.node_name), \
                        parent=self.tree.tree_root,move=self.game.last_move, \
                        loss=0,win=0,draw=0,player=-1*self.player)
        				else:
        						self.tree.tree_root=node_tmp
        		node_point=self.tree.tree_root #5
        		for i in range(try_times): #6
        				board_=copy.copy(self.game.board.board)	
        				node_start=node_point
        				move_records,game_result=easySimuGame(board_,self.player)
        				for move in move_records:
        						node_start=self.tree.updateLeaf(node_start, \
                        move,game_result) #7
        		all_next_leaves=self.tree.getNodeLeaves(node_point)
        		all_rates=[(node.loss/(node.loss+node.win+node.draw)) \
                for node in all_next_leaves]
        		all_rates=np.array(all_rates)
        		pick_move_index=int(random.choice(np.argwhere( \
                all_rates==min(all_rates)))) #8
        		self.tree.tree_root=all_next_leaves[pick_move_index] #8
        		return all_next_leaves[pick_move_index].move
  
```
{% endcode %}

1. 设置模拟对弈局数；
2. 判断是不是刚开始下棋；
3. 查找当前局面是否在过去的仿真中出现过；
4. 如果第一次见到此局面则将其添加到完整的树结构中；
5. 根据当前局面从搜索树中剪出需要的树枝，抛弃不相关的分支；
6. 采用随机策略多次仿真当前局面下双方落子；
7. 根据仿真结果更新各节点得分信息；
8. 选择得分最高的节点作为当前局面下的最优估计。

鼓励读者尝试在ttt.py中选择不能的智能AI，观察一下对战结果。使用蒙特卡洛的智能程序在每步模拟1000次的情况下，棋力基本上就极小化极大和使用算法的智能程序一样了，而且在计算耗时上更平滑，初始几步的耗时要明显优于极小化极大算法。

### 3.3.3蒙特卡洛算法改进

蒙特卡洛搜索树对模拟采样的要求较高，越多的模拟代表消耗更多的计算资源，但是对于对弈游戏来说，很多时候一些落子位是明显没有意义的。井字棋为例，对于人类选手，一眼就会看出图中★标注的位置是无论如何不能落子的。而对于单纯的蒙特卡洛算法，由于没有关于游戏的先验知识，算法会把这些★的位置一并纳入随机采样中，这不仅是对计算资源的浪费，同时也会给游戏的对弈体验带来负面影响，因为AI计算耗费的时间并没有和它的棋力对等。

![](.gitbook/assets/y1%20%281%29.png)

在围棋这种有很大的分支系数的对弈游戏中，为了提高蒙特卡洛算法的搜索效率，我们可以增加一些辅助算法来限制搜索的范围，辅助算法需要帮助改进：

* 不要搜索没有意义的节点，尽可能把计算资源放在搜索价值大的节点
* 在有限的时间里进行更多的仿真模拟

在处理围棋的死活时：

![](.gitbook/assets/y2.png)

执黑的人类棋手为了做活黑子，对画〇 的位置几乎是不会考虑的。另外和▲的位置相比，人类棋手会更倾向于先思考画✖的位置，因为这些位置从表面上看起来对活棋更有利。以模拟人类下棋思考模式的角度出发，我们为算法增加一个局面评估函数v=F\(x\)，函数的输入是当前棋局，输出是各落子点的潜在价值。函数的作用是辅助算法缩小仿真的范围。

![](.gitbook/assets/wei-ming-ming-hui-tu-14.svg)

如何得到评估函数v=F\(x\)？一般是由专家根据经验手工编辑各种特征规则，函数将输入的局面去匹配这些特征规则后给出评分。如果没有专家，也可以通过机器学习，根据大数据样本学习出特征规则。需要注意，评估函数仅仅是辅助作用，它输出高评分的落子位置并不代表这个落子点一定是好的。参考评估函数给出各落子点的得分，蒙特卡洛算法优先选择得分高的点开始仿真。

很少有人类棋手对弈会下到最后不能落子了才投子结束棋局，大部分的围棋在下到中盘时就已经分出了胜负。采用蒙特卡洛算法的AI智能算法非常依赖仿真的次数，在有限的时间内，我们总是希望可以仿真更多的棋局。自然地，我们希望仿真算法也可以不用下完整盘棋，在差不多中盘的时候就提前评估棋局的输赢。

针对围棋，有一个很意思的评估方法叫做“动态系统局面评估”。这种评估方法是把黑白棋子看作类似正负电荷一样的物质，棋子对四周“辐射”自己的影响力，相同的棋子可以叠加自己的的影响力场，不同的棋子则相互抵消。动态评估试图在围棋棋盘上建立自己的物理法则，用来确定棋盘上每个棋子的势力范围，在Alpha-Go问世以前，很多主流的围棋AI程序都有使用这种思想。

![](.gitbook/assets/y4%20%282%29.png)

围棋的目的就是占领比对方更多的棋盘领地。利用动态局面评估机制，我们可以评估当前局面各方的势力范围，据此计算机程序就能粗略地判断棋盘的输赢。通常，我们为动态评估建立一个函数J=F\(x\)。F\(x\)输出局面的输赢以及对局面判断的置信度。和评估函数v=F\(x\)类似，动态评估的函数机制也可以人为编制。随着计算机大数据处理能力的增强，采用机器学习的方法获取J=F\(x\)会比人为设立规则更轻松简便。

为了得到样本棋局每一回合的胜负置信度，我们引入参数退化参数R\(0&lt;R&lt;1\)。当样本局结束时，胜负判断的置信度W为100%。之前的每一回合通过参数R反向传递置信度，可以得到：

$$
R=1/n   (n=当前棋局的回合数)
$$

$$
W(T-1) = W(T)-R
$$

![&#x53CD;&#x5411;&#x4F20;&#x9012;&#x80DC;&#x7387;&#x7F6E;&#x4FE1;&#x5EA6;](.gitbook/assets/wei-ming-ming-hui-tu-17.svg)

可以人为设置一个置信度阀值，当J=F\(x\)超过这个阀值时，则提前判定胜负，结束一次仿真。

有了辅助函数v=F\(x\)和J=F\(x\)，蒙特卡洛搜索树不仅可以控制搜索范围，还可以限制单次仿真的搜索深度，采用这种启发式的方法，算法可以在有限时间里把计算资源尽可能地用于计算有价值的落子选择，对最有可能胜利的落子范围进行仿真。如果您读过金庸先生的《[天龙八部](http://ds.eywedu.com/jinyong/tlbb/mydoc032.htm)》就会知道，在围棋的世界里，表面看上去占优势的下法未必会赢，而前期劣势的下法可能会在后期转化为巨大的优势，如果只根据评估函数一味地考虑优势下法可能并不是一个好的选择。比如下图这个局面：

![](.gitbook/assets/y5.png)

对于白棋而言，评估函数如果偏重考虑做活自己的区域，标〇的位置会得到比较高的评分。虽然白棋在这些区域落子后可以大概率将下半部分棋盘纳入自己的势力范围，但是对于全局而言，黑棋则有机会占领更多的上半部分棋盘，最终由于白棋在下路下的太厚，导致输掉这盘棋。如果白棋在评估函数以外，能考虑到画✖的位置，破坏黑棋独占上半路棋盘的趋势，则会有更大概率赢得这盘棋。

蒙特卡洛搜索树的改进UCT算法出现的目的，就是在树搜索的深度与广度之间找到一个平衡。2006年秋季，两位匈牙利研究人员列文特·科奇什和乔鲍·塞派什瓦里开发了UCT算法，使得围棋智能程序的胜率比当时的最佳算法提高了5%，并且能够在小棋盘的比赛中与人类职业棋手抗衡。

![](.gitbook/assets/wei-ming-ming-hui-tu-9-1.svg)

在评估函数的辅助下，搜索算法总是企图从价值最高的点开始模拟，这会导致算法偏重评估某几个节点，忽略了一些潜在选择项。对于围棋而言，一些落子点的效果很可能要延迟很多回合后才会发挥威力，这些位置的价值在一开始不能通过固定的特征值组合判断出来。所以搜索树在仿真时不能一味地沿着黑色节点前进，并且只考虑评估得分高的灰色节点作为继续的仿真对象。对评估价值低的点也应当偶尔提供计算资源。我们使用UCT公式来平衡搜索深度与广度之间的关系：

$$
v'=ω+c\sqrt{\frac{logN}{n}}
$$

ω是原始的价值函数v=F\(x\)，N是本次AI计算时计划仿真的总次数，n是当前被考虑的节点计划仿真的次数，c是人为选择的一个平衡参数。新的价值函数v'用来代替原始的价值函数，搜索仿真依照新的各节点得分来进行选择。当人为参数c选的比较大时，评估函数偏重将计算资源投入到原本价值不高的节点上，c比较小时，仿真则会偏向从评估价值高的节点上开始。

![](.gitbook/assets/wei-ming-ming-hui-tu-9-5.svg)

根据上图的计算，在使用新的价值函数后，原本被忽略的节点反而被重点考量。超参c应该如何选择才能达到好的效果可能仁者见仁智者见智。需要注意，由于算法调整了原始的评估价值，对应的仿真次数也应重新分配。在总仿真次数固定的情况下，根据评分归一化后均匀分配仿真次数是可行的方案之一。如果计算资源充裕，采用动态分配也是理想的方法。

蒙特卡洛算法默认采用随机采样作为缺省的仿真策略。随机策略保证了采样的均匀性，但是在实际应用中，仿真有时间限制，采用随机策略的围棋AI往往棋力不高，为了进一步提高仿真的有效性，可以考虑使用更复杂的仿真策略，比如策略中可以考虑气 、形 、定式 、攻击模式  、防守模式等一些重要的围棋基本概念。比如在搜索策略中考虑引入常见棋形的固定下法，人类选手在评估棋力的时候，能记住多少围棋定式也是衡量标准之一。

![&#x9ED1;&#x68CB;&#x6253;&#x5403;&#xFF0C;&#x767D;&#x68CB;&#x957F;&#x7684;&#x56FA;&#x5B9A;&#x4E0B;&#x6CD5;](.gitbook/assets/wei-ming-ming-hui-tu-15.svg)

![&#x56F4;&#x68CB;&#x8D77;&#x624B;&#x603B;&#x662F;&#x4ECE;&#x89D2;&#x4E0A;&#x7684;&#x661F;&#x4F4D;&#x5F00;&#x59CB;](.gitbook/assets/wei-ming-ming-hui-tu-16.svg)

关于实现更复杂的仿真策略，读者可以参考开源的围棋AI引擎，[Gnu Go](https://www.gnu.org/software/gnugo/)、[Fuego](http://fuego.sourceforge.net/)和[Pachi](http://pachi.or.cz/)是其中的佼佼者。

### 3.3.4需要注意的问题

对一个可能有希望的落子点随机仿真10次后有7次赢得棋局，我们有多大的信心认为这个落子点是一步好棋呢？答案是：并不乐观。假使实际上这个落子点对胜负的影响是平衡的（无影响的），通过随机的方法我们也会有30%的机会得到7胜3负的结果。但是如果是100局仿真中有70次胜利呢？这种情况下，我们可以很自信地相信当前仿真的落子点是一步好棋。由于围棋的分支实在太多，对当前局面的下一步棋来说，一般需要仿真近千次才会有一些自信判断是否是一步好棋。通常留给计算AI程序思考的时间有限，而在一回合内至少需要仿真一万次以上才能基本上满足算法给出最优判断，此种情况下程序实现需要尽可能的优化，如果仿真一局都要耗时好几秒钟，即使算法再好，其实用性也会大打折扣。从提升性能的角度出发，如果仿真程序在每一回合都要对所有可选分支进行价值评估也是一笔不小的计算开支。如果仿真策略不使用随机策略，而是采用人工编制的高级策略，可以考虑不用每一步都计算各节点的评价得分，对于自信度高的下法可以跳过计算评估的动作。但是无论如何优化和改进，UCT算法只能在分支少的小棋盘上赶上人类选手，在19路围棋棋盘上依然还是初级选手的水平。

## 3.4 监督学习

机器学习能够在近些年获得如此迅猛的发展，和人们能更便捷地获取大量的数据是密不可分的。我们接下来要实现的基础智能AI也同样需要大量的数据。作为AI的学习教材，可以从 [_`https://www.u-go.net/gamerecords/`_](https://www.u-go.net/gamerecords/) 获取历年来在KGS上7段以上选手之间的对弈棋谱。网站上有“.zip”、“.tar.gz”和“.tar.bz2”三种格式。为了方便，我们在windows上处理这些数据，因此，我们只使用“.zip”格式的文件。如果你愿意，完全可以手工逐个点击下载，不过为了方便和快速，这里提供一个Python小程序，方便地获取所有的“.zip”格式链接。 右键浏览器，把网页文件保存在myGO\SGF\_Parser文件夹下，使用默认文件名“u-go.net.html”保存。 编辑Python文件：

{% code title="myGO\\SGF\_Parser\\fetchLinks.py" %}
```python
from bs4 import BeautifulSoup
f = open('u-go.net.html', 'r')
html=f.read()
soup=BeautifulSoup(html,"html.parser")
for link in soup.find_all('a'):
    if 'zip' in link.get('href'):
        print(link.get('href'))
```
{% endcode %}

在cmd窗口里，执行`python fetchLinks.py > zip.link` 打开“zip.link”文件，将全部内容复制后粘贴到迅雷中下载，文件请保存在“myGO\SGF\_Parser\sgf\_data\”。 全选所有下载下的zip文件，右键，选择7-zip进行解压，选择“提取到当前目录”，这样，在“myGO\SGF\_Parser\sgf\_data\”目录下就会有全部待解析处理的sgf文件了。

## 3.5传统方法的讨论

在国际象棋更加流行的西方，人工智能的研究在开创之初，就有人考虑如何制作一个能够下国际象棋的机器。20世纪80年代初，贝尔实验室的工程师们开发出了历史上第一个具有人类大师级水平的国际象棋机器“Belle”。Belle 由三个主要部分组成：移动生成器，评估器和变换表。移动生成器识别出遭受攻击的最高价值部分和最低价值部分，并根据该信息对潜在的移动行为进行排序。评估器会注意到国王在比赛的不同阶段的位置及其相对安全性。变换表包含了潜在移动的内存缓存，它可以使评估更加有效。Belle 采用了暴力的方法。它查看了玩家用当前配置的棋盘可能做出的所有动作，然后考虑了对手可以做出的所有动作。在国际象棋中，一名玩家移动一个棋子称为一层。最初，Belle 可以计算 4 层深度的移动。当 Belle 于 1978 年在计算机协会北美计算机国际象棋锦标赛上首次亮相时，它的搜索深度为 8 层，并夺得了冠军。

在 Belle 统治计算机国际象棋世界多年之后，它的明星效应开始褪色。80年代末，卡内基梅隆大学的许峰雄博士在 “Bella”的思路基础上进一步改进，研制出了第一个特级大师水平的国际象棋机器，取名“深思”。随后，许博士加入IBM研究院，在那里他和其他几个团队成员一起研制出了实力更强的弈棋机器“深蓝”，并最终于1997年的一场历史性的人机大战中以3.5：2.5的比分战胜了人类国际象棋冠军卡斯帕罗夫。深蓝并没有对传统的暴力算法进行太多的改进，客观地来说，深蓝不是基于AI技术构建的，它是一组专门为国际象棋而设计的CPU集群。深蓝也有棋类游戏智能软件常见的配置：开局库、着法生成器、评价函数、剪枝算法等。为了追求极致的搜索速度，它没有通过软件方式来实现，而是依靠专门为国际象棋设计的计算芯片，通过每秒计算两亿步的恐怖算能把人类甩了在后面。

2000年开始，随着计算机硬件技术的发展和研究者对Alpha-beta算法进一步的研究和优化，越来越多的棋类对弈软件开始逐步超越人类。国际象棋的智能软件已经不再依赖专用象棋芯片就可以轻易战胜人类特级大师。国人制作的中国象棋软件也同样达到了无人匹敌的地步。这些弈棋计算机AI的基本“思考模式”很简单，就是对当前局面下的每一种合法走法所直接导致的局面进行评估，然后选择“获胜概率”最高的局面所 对应的那个走法。可以说，计算机的基本策略是所有“人类有可能采用”的策略中最原始最简单的一种。这些弈棋AI只关心 一个问题，就是按照“胜”的基本定义来赢得比赛。“在当前局面下，我走哪一步能赢”，计算机就是通过不停地重复问自己这个问题来完成对弈 的。

对计算机而言，从给定盘面开始的局势变化的复杂度是随考虑的步数呈指数级增长的。对于包括围棋和象棋的绝大多数复杂棋类运动而言，这意味着从原则上不存在准确计算盘面的最优结果的有效方法。下表列出了常见棋类的状态空间复杂度和博弈树复杂度。状态空间复杂度排除了不符合游戏规则的情况，但是实际在计算机AI搜索策略时，需要面对的是博弈树展开后的复杂度。

| 游戏名称 | 状态空间复杂度 | 博弈树复杂度 |
| :--- | :--- | :--- |
| 西洋跳棋 | 10^21 | 10^31 |
| 国际象棋 | 10^46 | 10^123 |
| 中国象棋 | 10^48 | 10^150 |
| 围棋 | 10^172 | 10^360 |
| 黑白棋 | 10^28 | 10^58 |
| 将棋 | 10^71 | 10^226 |

可以看出，跳棋这种规则简单，步数有限的棋类游戏，其合法的状态空间复杂度都有10的21次方之巨。对于弈棋AI的设计者来说，“不可能对局势变化的所有可能性进行有效计算”意味着想做得比对手更好需要从原理上解决两个关键问题： \(1\)决定一个“筛选策略”，从所有从当前盘面出发有可能导致的变化中选择一部分作为“我们实际考虑的那些局面变化”；\(2\)决定一个“汇总策略”，把所有实际考虑的变化的静态评估结果综合起来，对当前盘面的胜率完成评估。

归根结底，传统算法的目的就是尽可能正确的”打分“和尽可能多的”穷举“。无论是国际象棋、中国象棋、围棋、或者其他如西洋跳棋、黑白棋，所有棋类程序在Alpha-Go出现以前，都围绕着这两点来进行基本框架的搭建。它们之间的区别仅在于使用的具体策略不同，以及针对不同的策略采用不同的优化手段。传统的方法在搜索复杂度不是特别高的情况下可以工作的很好。比如中国象棋和国际象棋的AI程序早在十几年前就已经赶超了人类最顶尖的选手。不知道为什么，同样的策略作用在围棋软件上效果却泛善可陈。也许是围棋规则太过简单，又或者是在下围棋的时候并不是时时刻刻都要秉承着其最终胜利的原则。 汉代扬雄在其《法言·君子》中提到：“昔乎颜渊以退为进，天下鲜俪焉”，意思是：以谦让取得德行的进步，以退让的姿态作为进取的手段。传统方法时时刻刻都在基于“胜”的定义进行思考，也许可能太过激进。

Alpha-Go背后的强化学习策略一改传统的设计思路。 它不是一开始就奔着设计出一个强大的AI程序为目的，强化学习只教给AI程序基本的游戏规则，然后让程序自己在游戏中学习， 这种方法目前看起来更贴近人类的学习的路径，效果也更好。





















