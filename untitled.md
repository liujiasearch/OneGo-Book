---
description: 这一章概要地介绍了传统棋类的AI算法，主要是极大极小算法、Alpha-Beta剪枝法和蒙特卡洛搜索树，并通过井字棋游戏来具体说明算法的实现方法。
---

# 第三章 传统的棋类智能

与其它棋类游戏相比，计算机模拟的围棋人工智能棋力进展相对缓慢。在1997年，就有计算机可以击败世界国际象棋棋王卡斯帕罗夫，但围棋软件直到2016年才第一次真正地击败顶尖围棋棋手。国际象棋目标明确，只要杀死国王即可（中国象棋、日本的将棋状况也差不多），因此算法较为简单。但是围棋以“围”为目标，不一定需要杀死对方棋子，每一步有数百种以上的走法，因此围棋的复杂度高，又极具欺骗性，这些因素都对计算机博弈程序提出了巨大的挑战。

虽然像IBM深蓝那样的超级计算机已经能够击败世界上最好的国际象棋棋手，但在AlphaGo出现之前却有不少人能轻易击败当时的围棋软件。黄山谷有诗：“心似蛛丝游碧落，身如蜩甲化枯枝”，可见围棋给程序员们带来了许多人工智能领域里的挑战，要编写出超越初级水平的计算机围棋程序都是一件极其困难的事。

过去的传统方法一直没有能够在围棋上有所突破，但是在象棋等其他棋类上还是展现出了其强大的算法实力，了解这些内容，我觉得是有益的。现代基于神经网络的人工智能方法需要巨大的计算机算能，当我们没有足够的算能来训练出一个强大的棋类智能体时，尝试利用传统方法不失为一种变通之法。这就有点像牛顿力学与爱因斯坦相对论之间的关系，大部分时间，我们使用牛顿力学就好了。传统方法的一个致命问题是算法中带有太多人类的主观想法，在谈论细节时，我想读者会有更实际的感受。

## 3.1 极小化极大（Minimax）算法

极小化极大算法常用于棋类等由两方较量的游戏和程序，它可以追溯到中世纪，属于是一种找出失败的最大可能性中的最小值的算法。算法的基本思想是假设游戏的获利与损失总是零总和的，从当前参与博弈的角色角度来看，该角色总是在可选的选项中挑选将自己的优势最大化的选择，而对手方总是选择令自己优势最小化的方法。很多棋类游戏可以采取此算法，例如井字棋。对于交替博弈的双方而言，如果站在各自的角度来看，当前博弈方总是挑选使得自己优势能够最大化的选项。两种看上去矛盾的描述和解释，仅仅是因为我们所选择的当前博弈对象不同而已。

极小化极大算法顾名思义，就是让最大的情况最小，这里的最大一般是指最差的情况，例如游戏中最不利的情况。换个角度来理解就是说下棋时己方要时刻保证最小化对手方的最大收益。 这个算法的前提是游戏必须要满足零和博弈的条件，即两个玩家进行博弈，如果其中一方得到利益那么另一方就会失去利益，游戏利益的总和为零或者是某一个常数，如果不满足这个条件，算法就将失去意义。本质上极小化极大算法就是一个树形结构的递归算法，每个节点的子节点和父节点都是对方玩家，所有的节点被分为两类，分别是我方的极大值节点和对方的极小值节点。

极小化极大算法一般都会通过递归来实现，主要原因是递归方法在代码编写上可以做到简单明了。如果读者觉得理解递归逻辑在思维上有困难，也可以通过展开博弈树来实现算法，但是后者在代码复杂度上要远远超过前者。我们的讲解也是基于递归算法的。前面提到过算法本质上是”己方要时刻保证最小化对手方的最大收益”就属于递归的思想，也就是说，判断己方收益的方法是基于对方的收益。比如要计算A的收益，就要先计算B的收益，要计算B的收益，就要再计算A的收益，往复循环直到满足循环的结束条件，这就构成了递归的基础。这里以井字棋为例来实现一个会玩井字棋的智能程序。

井字棋是一种在三排棋盘上进行的连珠游戏，它需要两个玩家进行游戏，玩家双方轮流在三乘三的格上打自己的符号，一个负责在格子上打圈（〇），一个在格子上打叉（✖），最先在横、直或斜其中任一条件下将自己的符号连成一线的玩家为胜。井字棋游戏只有765个可能局面，26830个棋局。如果将对称的棋局视作不同局，则它有255168个棋局。

![&#x56FE; 3 - 1 &#x8FDB;&#x884C;&#x4E2D;&#x7684;&#x4E95;&#x5B57;&#x68CB;&#x6E38;&#x620F;](.gitbook/assets/image%20%2820%29.png)

假设当前井字棋游戏下到如图3-1所示的状态，现在轮到画✖方落棋，显然下到右下角就能赢取胜利。要计算机程序利用极小化极大算法进行井字棋游戏，我们首先就需要为这个AI程序定义一个知道怎么在下一步获胜的函数。

{% code title="伪代码：" %}
```python
def findOneMoveWin(game_state,player):
    possible_moves = []        
    for candidate_move in game_state.legal_moves(player):    #1
        next_game_state = game_state.apply_move(candidate_move)    #2
        if next_game_state.is_over() and next_game_state.winner == player:
            possible_moves.append(candidate_move)    #3
    return possible_moves    #4
```
{% endcode %}

1. 遍历当前玩家所有的合法选择；
2. 得到每个选择带来的结果；
3. 如果当前选择能带来胜利，就保存起来；
4. 遍历完后返回之前保存的可以赢取棋局的落子选项。

通过以上算法就能轻易看出一个弊端，仅仅是要想知道哪一步一定能取胜，就需要遍历所有可能的选择。这种方法在象棋这种有固定棋子和固定下棋方式的游戏中或许可行，但是在围棋中实在是太困难了，这种算法所带来的计算量将是灾难性的。

![&#x56FE; 3 - 2 &#x5012;&#x9000;&#x4E00;&#x6B65;&#x68CB;](.gitbook/assets/image%20%283%29.png)

但是仅仅知道怎么取胜还是远远不够的。让我们想想，双方是怎么会下到图3-1这个局面的呢？我们把游戏倒退一步，回到图3-2的样子。执✖方选择了左上角和中心的位置，执〇方现在也占据了左下角。现在轮到执〇方走一步，他会很天真地认为如果自己占据了下路的中间位置就能在后一步中连成一线吗？这似乎是很可笑的，因为执✖方会比他先连成一线。那么显然我们的智能程序必须要知道在自己取得胜利之前不能给对方任何获胜的机会。具体到井字棋游戏就是要占据对方能够获胜的位置，即如果己方当下不能立即赢取胜利，那么就必须要阻止对方在接下去的一步棋中获得胜利。由此我们还需要定义一个能避免自己失败行为的函数。

{% code title="伪代码：" %}
```python
def findMinLoseMoves(game_state,player):
    opponent = player.other()    #1
    possible_moves = []
    for candidate_move in game_state.legal_moves(player):    #2 
        next_state = game_state.apply_move(candidate_move)    #3
        opponent_winning_move = findOneMoveWin(next_state, opponent)    #4
        if opponent_winning_move is None:
            possible_moves.append(candidate_move)    #5
    return possible_moves
```
{% endcode %}

1. 初始化己方的对手；
2. 遍历自己所有的合法选择；
3. 计算执行该选择后留给对方的新局面；
4. 对方能否在这个新局面获胜；
5. 如果不能，加入己方可选项的集合内。

显然我们的程序在调用`findMinLoseMoves()`这个函数前需要先调用`findOneMoveWin()`，只有知道自己不会在下一步中取胜才去思考阻止对方胜利的选择，因为一旦胜利了，游戏也就结束了。己方既然能够找到防止对方胜利的选择，相反地，对方也会使用这种相同的策略，所以无论如何选择，对方总有应对之策。按这个逻辑，在零和游戏中单纯模拟寻找胜利的选择是没有意义的，因为算法总是能保证在对方选择必胜的行动前就已经扼杀了这个选项。

![&#x56FE; 3 - 3 &#x591A;&#x60F3;&#x4E24;&#x6B65;](.gitbook/assets/image%20%2815%29.png)

如果双方在各自的当前回合都无法必胜，我们就要继续搜寻下去，看看是不是存在对方至少要连下两步棋才可以阻止己方胜利的情形。如图3-3，如果执✖方落棋到中心红色X点，那么执方〇必须同时堵住两个红色的画圈位置才可以避免执X方的胜利。根据这个想法，我们再实现一个函数`findTwoMovesWin()`，在调用该函数之后，对它返回的可选项进行筛选。

{% code title="伪代码：" %}
```python
def findTwoMovesWin(game_state, player):
    opponent = player.other()
    for candidate_move in game_state.legal_moves(player): 
        next_state = game_state.apply_move(candidate_move) 
        good_responses = findMinLoseMoves(next_state, opponent)    #1
        if not good_responses:    #1
            return candidate_move
    return None
```
{% endcode %}

1. 这一步是整个函数的关键，逻辑上却很简单。如果己方当前选择使得对方在下一步无法阻止己方在下下一步获胜，那么当前一步棋必然是制胜棋。

如果一方的程序能找到一步必胜棋，逻辑上对方应该已经在上一步行棋时占据了这个位置，或者提前扼杀了这种可能性。所以如果可以穷举，依照算法的逻辑，任何一局棋从一开始就已经注定了谁胜谁负。不仅是井字棋，几乎任何棋类，只要先下的一方如果不出现失误，这一局就起码是和局，也就是在一开始提到的零和。

再回顾一下上面这个算法中AI考虑的完整思路：

* 当前步能不能立即胜利，如果能就执行；
* 能不能阻止对方在下一步胜利，如果不能就认输；
* 对方下完后能不能找到自己在后续一步中立即胜利，如果能就执行；
* 查看对方是不是在下一步存在必胜的下法，如果不能就认输；
* ......
* 能不能找到自己在下一步中必胜的下法，如果能就执行；
* 查看对方是不是在下一步存在必胜的下法，如果不能就认输；
* ......

从上述的算法我们可以看出，整个算法即简单又粗暴，计算过程试图穷尽所有可能的选择，如果己方下一步不能胜利，就尝试阻止对方在下一步胜利，之后再考虑己方在下下一步能否胜利，如果不能则尝试阻止对方在下下一步的胜利，以这种逻辑不断地模拟行棋的过程，只要己方在落子时不能胜利，就将子落在能阻止对方胜利的位置，直到达到棋局终止的条件。即便是像井字棋这种非常简单，仅有9个落子位的棋类游戏，第一步行棋要搜索的空间也达到94981步，虽然对现代计算机而言这算不上是很深的深度，但如果是象棋游戏，这个数字就会变得非常可怕。以现代计算机的处理能力，这种贪婪算法所耗费的时间是不可接受的（计算一步棋大概要算上好几年）。假使换做是围棋，这个算法就会更加耗时冗长，几乎是会永无止境的运算下去。

![&#x56FE; 3-4 &#x6781;&#x5C0F;&#x5316;&#x6700;&#x5927;&#x7B97;&#x6CD5;&#x7684;&#x987A;&#x5E8F;&#x601D;&#x7EF4;&#x8FC7;&#x7A0B;](.gitbook/assets/sin%20%2821%29.svg)

虽然极小化极大算法在效率上不太理想，但是对于井字棋，它已经足够好了。最后为了完成这个AI程序，我们还需要使用一个小技巧，因为之前的伪代码是按照图3-4的顺序逻辑来写的，如果简单地照抄我们必须手工编写每一步判断函数。当然如果我们只考虑未来的有限步，这种方式勉强还是可行的，不过在一开始也说了，实现这种算法还得使用递归这种编程技巧，因为递归过程并不需要指定探索深度，只要计算机内存足够运行时就不会报错。在介绍这个小技巧前，我们先定义几个游戏的枚举类和一些框架性的类方法，这和在上一章所做的工作类似，定义枚举类仅仅是为了使得代码更加可读，也为了编程时方便对变量进行记忆。

{% code title="MyGo\\tic-tac-toe\\main.py" %}
```python
class GameResult(enum.Enum):    #1
    loss = 1
    draw = 2
    win = 3
class GameState(enum.Enum):    #2
    waiting=0
    running=1
    over=2

player_x=1    #3
player_o=-1
```
{% endcode %}

1. 井字棋的结局存在胜、负与和棋3种结果。为了后面对棋局优劣的判断方便，对枚举值的赋值按输到赢，数值安排由小到大，这样安排的便利性将在后面实际编程中体现；
2. 定义游戏的状态，主要是为了方便AI知道游戏什么时候结束；
3. 为了编程方便，没有为执棋的双方再定义一个枚举类，而是直接为其赋了变量值，这在实践上并不是一个好的习惯，但是目前在我们的演示中这不会是一个问题。

和大部分棋类一样，我们把下棋这件事拆分成3个主要的对象类。棋盘和下棋的人是显而易见的实体，而棋局的状态属于一个比较抽象的概念，简单来说，将它实例化后就可以代表一局棋。现实生活中当我们有了棋盘有了下棋的人，剩下的事情就是下棋了。我们下第一局，第二局，第三局等等直到厌烦，而这每一个具体的局棋就看作是棋局状态类的一个实例。我们还可以额外定义一个裁判类，不过由于井字棋规则简单，这里就不再额外定义这个类了，仅是把裁判的功能放在棋局的状态类里实现。

先来看一下抽象后的棋盘：

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
class Board:
    def __init__(self):    #1
        self.board=np.zeros((3,3))
    def printBoard(self):    #2
        for i in range(3):
            line='|'
            ele=[]
            for j in range(3):
                if self.board[i,j]==-1:
                    ele.append('_O_')
                elif self.board[i,j]==1:
                    ele.append('_X_')
                else:
                    ele.append('___')
            print(line.join(ele))    #3
```
{% endcode %}

1. 将井字棋的棋盘初始化为一个3×3的全零numpy数组，用1表示画X，-1表示画O；
2. 定义一个打印棋盘的方法，方便人机交互；
3. 按行来打印棋盘。

面向对象的编程有个特点，每个人都可以对要实现的事情有各自不同的认识与抽象结果。这里我把Board类抽象的很简单，就是一个棋盘，我们能做的就是通过打印方法去查看它，这也与实际生活相符，如果你愿意，也可以给它加上落子的方法来更新`Board.board`的盘面，但是我打算把更新`Board.board`这件事交给别的类来做。

和智能算法无关的事情，我打算都交给记录棋局的状态类来完成，接着我们来看看这个类需要做些什么：

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
class Game:
    def __init__(self, board, player=player_x,game_state=GameState.waiting):
        self.board=board    #1
        self.player=player    #2
        self.winner=None
        self.state=game_state
        self.bot1=None
        self.bot2=None

    def getResult(self):    #3                   
        ...
    def run(self,mode='hvh',bot1_mode='r',bot2_mode='r'):    #4
        ...
    def applyMove(self,move):    #5
        ...
    def simuApplyMove(self,move):    #6
        ...
    def isLegalMove(self,move):    #7
        ...
    def getLegalMoves(self):    #8
        ...
```
{% endcode %}

1. 把棋局和棋盘挂钩，表示某一局棋下在哪副棋盘上；
2. 记录当前回合属于哪一方；
3. 判断当前棋局状态，这个方法将返回之前定义的枚举类`GameState`的值；
4. 用`run`这个方法来启动游戏，执棋双方在这个方法中轮流下棋，其中mode用来提示下棋模式，游戏可以支持人与人的对战，人与AI的对战以及AI和AI之间的对战；
5. `applyMove`用来更新棋盘，读者也可将这个方法放在Board类中；
6. `simuApplyMove`做的事情和`applyMove`类似，只不过一个是真的在棋盘上落子，一个只是模拟AI在思考时的虚拟落子。`simuApplyMove`这个方法会额外返回一个Game实例，因为我们不能在原棋盘上更新思考时假想的落子，这一点和人类在下棋时的行为是类似的；
7. 判断落子是否合法。井字棋游戏不允许在已经下过的位置再下棋，一般也可以为这种游戏规则的校验再额外设置一个裁判类，使得逻辑抽象和分工更加明确；
8. 这个类返回当前棋局状态下所有合法的选择，这个方法也可以放在Agent类中实现，但是为了保持风格一致，我选择了尽量简化另外两个类。

上面这些方法都和本章的智能主题不太相关，所以具体的实现可以访问MyGo在github上的源码，源码里有更详细的注解，这里就不再赘述了。

和棋盘类一样，我不想把下棋的人这件事情抽象的过于复杂，AI程序只需要能根据棋盘的当前状态给出下一步出棋就行了。

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
class Agent:
    def __init__(self,game,player,mode='r'):     
        self.game=game    #1
        self.player=player    #2
        self.mode=mode    #3    
    def chooseMove(self):
        if self.mode=='r':    #4                       
            moves=self.game.getLegalMoves()
            return random.choice(moves)            
        if self.mode=='ai':    #4
            ...
```
{% endcode %}

1. 把AI和具体的游戏实例挂钩，相当于告诉AI，它在下哪盘棋；
2. 给AI分配角色，告诉它是执X方还是执O方；
3. AI可以有很多不同的智能算法，我们用mode来告诉它应该使用哪种算法；
4. 定义出棋的方法，这里我们先提供两种方法，‘r’表示随机落子法，‘ai'表示采用极小化极大算法。

随机方法的棋力非常弱，它从所有合法的选项中随机挑选出一个选择，专门实现随机方法这件事的目的是为了给后面的极小化极大算法提供一个参考对手，我们后面会看到贪婪算法与随机算法在棋力方面的差距。我们先来着重看一下极小化极大算法在实际中的是如何实现的。

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
        if self.mode=='ai':
            moves=self.game.getLegalMoves()    #1
            win_moves=[]    #2
            loss_moves=[]    #2
            draw_moves=[]    #2
            for move in moves:
                new_game=self.game.simuApplyMove(move)    #3
                op_best_outcome=bestResultForOP(new_game)    #4
                my_best_outcome=reverse_bestResultForOP(op_best_outcome)    #5
                if my_best_outcome==GameResult.win:
                    win_moves.append(move)
                elif my_best_outcome==GameResult.loss:
                    loss_moves.append(move)
                else:
                    draw_moves.append(move)
            if win_moves:
                return random.choice(win_moves)
            elif draw_moves:
                return random.choice(draw_moves)
            else:
                return random.choice(loss_moves)
```
{% endcode %}

1. 仅考虑所有符合游戏规则的落子选项；
2. 设置变量存放搜索出的必胜步、和局步和必输步；
3. 模拟当前选项在当前局面后的效果，之前提过，这个行为就是类似于人类选手在头脑中思考当前走某一步后的可能结果；
4. 这一步是极小化极大算法的核心，由于前一步虚拟落子后接下去是对方的回合，所以调用`bestResultForOP()`获取当前选项落子后，对手能得到的最好结果。这个和之前伪代码`findMinLoseMoves()`中的`opponent_winning_move = findOneMoveWin(next_state, opponent)`有异曲同工之妙；
5. 对手下一步能达到的最好结果的相反面就是己方当前局面可以取得的最好结果；

我们在编写极小化极大算法没有站在己方的角度来思考，而是站在了对方的角度来对棋局进行评价。正是这个技巧使得整个算法采用递归来实现变得具有可行性，否则我们只能采用像图3-4那样的循序逻辑来手工编写每一步落棋判断直到棋局结束。进入到`bestResultForOP()`内部查看源码会发现这个函数会调用`bestResultForOP()`本身，这也是递归写法的一个典型特征。如果己方要知道当前状态的最好结果就要查看对方在下一步情形下的最好结果，而对手想知道自己的最好结果就又要再看己方下下一步能够获得最好结果，如此往复循环直至游戏结束，即有一个明确的胜负或者和局的结果。

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
def bestResultForOP(game):
    if game.state==GameState.over:    #1
        if game.winner==game.player:
            return GameResult.win
        elif game.winner==None:
            return GameResult.draw
        else:
            return GameResult.loss
    best_so_far=GameResult.loss    #2
    for move in game.getLegalMoves():
        new_game=game.simuApplyMove(move)    #3
        op_best_outcome=bestResultForOP(new_game)    #4
        my_best_outcome=reverse_bestResultForOP(op_best_outcome)
        if best_so_far.value < my_best_outcome.value:    #5
            best_so_far=my_best_outcome
        if best_so_far==GameResult.win:    #6
            break
    return best_so_far    #7
```
{% endcode %}

1. 如果当前游戏状态已经结束了，则返回游戏的结果，递归的终止条件依赖这个判断；
2. 初始化当前局面能够获得的最好结果；
3. 模拟当前选项产生的新棋局。这个我们已经在外层的方法中看到过了，显然这是准备开始递归了。游戏的状态`state`在这个方法中更新，这个值控制着`bestResultForOP()`停止递归；
4. 递归调用`bestResultForOP()`查看对手的最佳结果；
5. 如果当前最佳结果有提升则更新该值；
6. 胜利是棋局的最佳结果，一旦找到了这步棋就可以退出查找了；
7. 返回当前玩家能获取的最佳结果。

## 3.2 Alpha-beta剪枝算法

根据前面的介绍可以发现，极小化极大算法会遍历所有的可能性，但是常识告诉我们，并不是所有的选项都需要进行深入的考虑，存在着某些明显不利的选项，当出现这种选项时就可以换一种思路进行考虑了。Alpha-beta剪枝算法的出现正是为了减少极小化极大算法搜索树的节点数。1997年5月11日，击败加里·卡斯帕罗夫的IBM”深蓝“就采用了这种算法。

![&#x56FE; 3-5 &#x6267;&#x3007;&#x65B9;&#x4E0B;](.gitbook/assets/sin%20%2823%29.svg)

以井字棋为例，我们先来看看在下棋的过程中是否有优化空间。参考图3-5，此时轮到画〇方，如果不在虚线圈上落棋，下一步画✖方画在虚圈处，游戏就结束了。当发现这类问题时，再去思考其它五个位子的落子收益其实是没有意义的，白白浪费了计算资源。

再来看个象棋的例子。如图3-6，此时轮到红方走子。将炮横在中路是一个非常具有杀伤力的下法，后续可能可以配合自己的马走出“马后炮”的杀招。但是如果走了这一步，自己的马将会被对方的车立即吃掉，这一损失实在是太大了，所以面对此局面，实战时基本我们只考虑如何走马以避免被车吃掉，其它的走子都不会再深入考虑的。

![&#x56FE; 3-6 &#x6267;&#x7EA2;&#x65B9;&#x7684;&#x9009;&#x62E9;](.gitbook/assets/sin%20%2822%29.svg)

在行棋的过程中，当发现己方会出现极大损失或者极大获利时，我们仅考虑这些收益显著的情况而忽略掉其它可选项的行为就是剪枝算法的基本思想，而Alpha-beta剪枝算法就是专门设计用来减少极小化极大算法搜索树节点数的搜索算法。它的基本思想是根据上一层已经得到的当前最优结果，决定目前的搜索是否要继续下去，当算法评估出某策略的后续走法比之前策略的还差时，就会停止计算该策略的后续发展。Alpha-beta剪枝算法将搜索时间用在“更有希望”的子分支上，继而提升搜索深度，则同样时间内搜索深度平均来说可达极小化极大算法的两倍多。

根据算法介绍可知，如果要使用Alpha-beta剪枝算法就会额外需要一套局面价值评估系统来决定哪线搜索分支是有希望的，而哪些是没有希望的分支。所谓局面价值，就是指当前盘面的胜负概率，胜率越高则价值越大，反之则价值越小，甚至是负价值。各种采用Alpha-beta剪枝算法的人工智能程序之间的实力差距其实就是由于局面价值评估系统的不同所造成的。局面价值评估系统带有很强的主观性，对于如何评估棋局的价值有点像莎士比亚说的，"一千个观众眼中有一千个哈姆雷特" 。下面将继续使用井字棋来演示Alpha-beta剪枝算法。为了省去设计井字棋的价值函数，我们粗暴地认为除了赢和输，其它所有盘面（包括和棋）的价值均为零，赢棋的盘面价值为一，输棋的盘面价值为负一。如果读者想自己在围棋游戏上尝试一下这个算法，最简单的局面评估算法之一就是计算当前双方在棋盘上剩余棋子的差额。不过实战中很少会有棋手主动提取对方已经穷途末路的棋子，所以也许这种评估方法得到的高价值局面反而会带来更加不利的影响。

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
def evl_game(game):
    if getResult(game.board.board)[1] != None:    #1
        if game.player == getResult(game.board.board) == (0,1)[1]:    #2
            return 1    #3
        else:
            return -1    #3
    else:
        return 0    #3
```
{% endcode %}

1. 判断盘面结果，按照约定，对于井字棋，只有当棋局胜负已分时才对盘面价值进行判断，否则盘面价值为零；
2. 判断当前进行价值评估的棋手是否是棋局的胜利方；
3. 如果胜利方是当前棋手，则盘面价值为一，如果胜利方是当前棋手的对手，则盘面价值为负一，其它情况的价值按约定默认是零。

引入了对棋局盘面的价值评估即表明我们在使用Alpha-beta剪枝算法时并不会执着于搜索时穷尽棋局，即在模拟思考行为时未必非要下到棋局结束时才停止。通常在使用这种算法时会设置一个搜索深度参数来控制算法仿真思考的回合数。从本质上来说，Alpha-beta剪枝算法是通过价值评估函数来控制算法的搜索广度，用参数设置来控制算法的搜索深度。

同极小化极大算法相比，Alpha-beta剪枝算法并不是要等到棋局下到结束才给出对局面的评估，每个不同可选项得到的评估结果会由价值评估函数给出不同的数值结果，不尽相同的评估结果（极小化极大算法只有胜、负、和三种评估结果）导致Alpha-beta剪枝算法在使用过程中需要记录博弈双方在搜索过程中所能取得的最佳价值，我们可以把双方记录的最佳价值等价地看作是极小化极大算法中的胜利结果。传统上把一方所能搜索到的当前局面最佳价值叫做Alpha，另一方的最佳价值称作Beta，这种叫法也正是这个算法名称的由来。对于井字棋，将其简记为`best_o`和`best_x`。

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
if self.mode=='ab':		#1
		moves=self.game.getLegalMoves()		#2
		best_moves=[]		#3
		best_score=None		#4
		best_o=minValue		#4
		best_x=minValue		#4
		for move in moves:		#5
		    new_game=self.game.simuApplyMove(move)		#6
		    op_best_outcome = alpha_beta_prune(
		    										new_game, max_depth,best_o,best_x,evl_game
		    										)		#7
		    my_best_outcome = -1 * op_best_outcome		#8
		    if (not best_moves) or my_best_outcome > best_score:		#9
		        best_moves = [move]		#10
		        best_score = my_best_outcome		#11
		        if self.game.player == player_x:		#12
		            best_x = best_score
		        elif self.game.player == player_o:		#12
		            best_o = best_score
		    elif my_best_outcome == best_score:		#13
		        best_moves.append(move)		#13
		return random.choice(best_moves)		#13
```
{% endcode %}

1. 模式ab代表Alpha-beta剪枝算法；
2. 获取当前盘面上符合游戏规则的可选项；
3. 存放最佳的落子选项；
4. `best_score`存放当前盘面在搜索过程中得到过的最高选项价值，这个值在搜索过程中会不断地被更高的值所替换；
5. 逐个搜索可选项；
6. 仿真一下当前选项的落子；
7. 仿真对手在当前落子下能取得的最佳价值；
8. 己方能取得的最佳价值是对方能取得的最佳价值的反面；
9. 只对当前价值高于已有记录的落子步进行处理；
10. 搜索到了更高的价值，于是需要更新最佳落子；
11. 更新已有记录的最佳落子价值；
12. 将最佳价值更新给当前棋盘盘面的实际落子方；
13. 如果搜索到的价值和记录的最高价值一致，则仅仅补充最佳落子的可选范围，通过随机抽取高价值落子使得下棋过程中棋局更多变，也更贴近人类行为。

上面的代码和极小化极大算法在框架上是非常相似的，如果读者仔细思索就会发现虽然算法的描述介绍好像有点玄乎，但是实现上Alpha-beta算法和极小化极大算法并没有什么本质上的区别，仅仅是将胜负结果的判断用一个价值判断函数替代了。既然Alpha-beta算法是对极小化极大算法的优化，它也只能通过递归的方式来实现。`alpha_beta_prune`函数是了整个递归方法的核心，读者可以将极小化极大算法中的`bestResultForOP`和这个`alpha_beta_prune`比较着来看。

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python

max_depth=4    #1

def alpha_beta_prune(game, max_depth,best_o,best_x,evl_fn):
    if game.state==GameState.over:
        if game.winner==game.player:
            return maxValue    #2
        elif game.winner==None:
            return 0
        else:
            return minValue    #2

    if max_depth == 0:    #3
        return evl_fn(game)    #3

    best_so_far = minValue    #4
    for move in game.getLegalMoves():    #5
        next_game = game.simuApplyMove(move)    #5
        op_best_result = alpha_beta_prune(    
            next_game, max_depth - 1,
            best_o, best_x,
            evl_fn)    #5
        my_result = -1 * op_best_result    #5
        if my_result > best_so_far:    #6
            best_so_far = my_result    #6

        if game.player == player_o:    #7
            if best_so_far > best_o:    #8
                best_o = best_so_far    #8
            outcome_for_x = -1 * best_so_far    #9
            if outcome_for_x < best_x:    #10
                break
        elif game.player == player_x:
            if best_so_far > best_x:
                best_x = best_so_far
            outcome_for_o = -1 * best_so_far
            if outcome_for_o < best_o:
                break

    return best_so_far    #11
```
{% endcode %}

1. 控制搜索深度。由于我们对平局和进行中的棋局的价值设置为0，而井字棋一共就9步落子，所以当这个搜索深度设置的比较浅时，算法在开头的几步和随机落子并没有什么区别。如果随机落子法在前3步完成了横竖相连，就可以击败我们的剪枝算法。这也从侧面说明了一个好的价值评估算法对于剪枝算法的重要性；
2. 由于采用价值评估函数来对胜负的可能性进行评估，这里用一个极大数字或极小数字来表示明确的输赢胜负；
3. 控制搜索深度，如果到达一定深度游戏还没有结束，就用价值评估函数的值来代替胜负的判断；
4. 和极小化极大算法一样，初始化当前局面能取得的最佳价值；
5. 这几步和`bestResultForOP()`中的写法是几乎相同的；
6. 如果结果比之前记录的好则更新最佳价值。极小极大化算法中的最佳价值就是赢棋，所以没有更新最佳价值这一步，而Alpha-beta剪枝中因为是通过价值评价函数来估计胜负结果的，这个值可能会有很多不同的值，所以可能需要不停地更新最大的值；
7. 根据当前执棋着是谁，将上一步得到的最佳值更新给不同的对象的最佳值；
8. 如果当前玩家是执〇方，当前搜索值大于执〇方记录的最大值，则更新其记录的最大值。下面对执✖方的判断后也使用了类似的操作步骤，就不再累述了；
9. 一方的最佳进行进行反操作就是另一方的最差；
10. 如果当前的一方最佳操作可以使得对方的最佳降低，那么就可以认为找到了一步必胜棋，并退出，当然也可以继续搜索不退出，但是由于已经找到了，再多找几个意义不大反而浪费了计算资源，这个在`bestResultForOP`中也有相似的对应操作；
11. 返回当前玩家所能取得的最佳结果。

搜索选项时我们会安排棋盘局面上的可落子顺序进行搜索。如果碰巧在一开始就找到了一个最好的选项，在搜索其它后续选项时会由于剩下的选项收益较低而被迅速地剪枝剪掉，如果运气不好，最好的选项在最后才被搜索到，那么Alpha-beta剪枝算法的速度并不会比极小化最大算法快。但是数学期望上，Alpha-beta剪枝算法的消耗时间会是极小化最大算法的一半。如果在搜索开始前引入一些启发性的算法先从最有潜质的着法开始搜索，也许可以缓解算法对着法寻找顺序的依赖并使得算法能有很大的改进。

## 3.3棋类局面评估

当把Alpha-beta剪枝算法的搜索深度设置的比较小时就会经常用到评估函数来估计盘面的胜负概率。对于我们刚刚实现的井字棋游戏来说，由于我们的评估函数非常弱，导致算法效果和随机落子没有什么区别。由此可见一个好的评估函数对于传统棋类是有多重要。

当博弈游戏比较简单，博弈树较小，可以完全展开时，每个子节点的价值都可以通过胜负结果来确定。对于稍微复杂一些的博弈棋类游戏，通常博弈树都很大，不能够被完全展开，即便采用了剪枝算法缩小了博弈树上的搜索规模，我们依然无法做到在有限的时间内完成全部分支的搜索，所以想让子节点通过胜负结果来确定价值也显得不切实际。通常的做法是限制搜索树的深度，当搜索到达一定深度时，博弈树子节点的胜负概率就需要通过评估函数来估计。“深蓝”使用了超过12层深度的搜索，12层以外使用了静态评估函数。

以象棋为例，由于游戏规则的限制，不同的棋子具有不同的价值。在数学模型上，我们可以为不同的棋子赋予不同的数值以表示其价值。我们用子力这个术语来表示棋盘上所有棋子价值的数值和。信息论的开山鼻祖香农博士曾经对国际象棋的棋子间相互博弈进行过分析，并给出了不同棋子的近似相对价值。

| 王\(K\) | 后\(Q\)  | 车\(R\)  | 象\(B\)  | 马\(N\)  | 兵\(P\) |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 200 | 9 | 5 | 3 | 3 | 1 |

根据表中的数值，我们可以得到棋盘当前子力的公式为：

$$
S=200(K-K')+9(Q-Q’)+5(R-R’)+3(B-B’)+3(N-N’)+(P-P’)
$$

其中K、Q、R、B、N和P表示白方的王、后、车、象、马和兵的个数，相对地，K‘、Q’、R‘、B’、N‘和P’表示黑方王，后、车、象、马和兵的数目。 仿照香农的做法，国人也给出了中国象棋棋子的近似相对价值。

| 将帅 | 车 | 马 | 炮 | 仕 | 相 | 兵 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| ∞ | 1000 | 450 | 450 | 170 | 160 | 60 |

围棋无法仿照象棋那样计算出子力，围棋的每个子本身都是一样的，每个子存在的价值取决于它和其它棋子之间的位置关系，很难通过象棋那种静态的方法对围棋的子力进行分析与评估，但存在有一些动态评估的方法。

棋子处于棋盘的不同位置时它的作用会差别很大。比如中国象棋里，中路车、过河兵与卧槽马都是指占据重要进攻位置的下法，具有很强的威胁性。而窝心马、沉底兵则是指棋子已经处在了不理想的位置，导致棋子本身的价值被削弱，无法发挥出来。所以仅考虑棋子本身的价值显然是不足的。在评估子力的时候，引入棋子的位置来对子力进行动态评估是非常必要的。对于围棋那种每个子都一样的游戏而言，这一点就凸显地尤为重要。

很多棋类游戏中，根据棋子相克的规则，我们可以把棋盘划分成多个不同的控制区域，包括本方控制区域、对方控制区域和公共区域。中国象棋在一开盘，就以楚河汉界为分界，将棋盘划分成两个势力范围。随着棋盘双方博弈进程的发展，控制区域也会随着发生变化。落入对方控制势力范围内的棋子将暂时失去价值。围棋本身以控制区域为目的行棋，势力范围的概念则更为明显。很多围棋动态评估的方法就是以控制区域为理论基础的。

象棋在棋盘上的子是灵活机动的，每个子的合法着法数目决定了它的机动性。例如中国象棋对马和象都有蹩腿的规则，所以它们的合法着法个数会跟着棋局的变化而减少。着法减少，机动性也就变低了。机动性越大，能控制的点就越多，影响就越大，选择有利己方局势的概率也就越大。围棋的棋子虽然落定后不能移动，但是如果棋子周围有比较开阔的空间，或者可以方便的和己方另外棋子相连，那么己方就可以比较方便的对外扩张，我们就认为其机动性较高；反之，如果一块棋被对方包围住，其对外延展被限制，我们就认为其机动性较小，能否存活就要取决于是否可以在有限的空间内做活。

一些对抗游戏的形势可能取决于着法的节拍或者游戏内容的数量关系。比如星际争霸这类RTS对抗类游戏，出招（有效点击鼠标）的频率大大影响着局势的发展。又或者是取豆子游戏，谁先拿到最后一个豆子的为胜。这类特征在围棋中展现的不是很明显，但是在一些其它的对抗博弈游戏中可能会需要考虑。

象棋中，如果本方的将帅已经在对方的威胁之下，评估其它棋子的实力意义就已经不大了，因为游戏规则逼迫本方必须响应对方。逼迫对方响应自己的下法是非常具有价值的，它使得对方仅有招架之功，并无还手之力。这种具有威胁性质的着法在围棋中最明显的体现是打劫，在打劫的过程中，双发都有极强的意愿去不断地寻找劫财，以免失去局部利益。

围棋形状的好坏，往往也是围棋博弈胜败的决定性因素。良好的形状有利于己方扩张地盘，有助于做活与连接。为了评估围棋棋形的好坏，人们发明了一些棋形提取算法与评估法。要使用这种算法，往往需要人工编辑和定义什么是好的棋形，什么是坏的棋形，且工作量巨大。图3-7演示了如何用棋形模版去匹配当前局面并输出一个综合评估得分。

![&#x56FE; 3-7 &#x56F4;&#x68CB;&#x68CB;&#x5F62;&#x7684;&#x641C;&#x7D22;](.gitbook/assets/1-2-.svg)

一个好的评价系统会综合考虑前面讲到的这些内容，目前看来，象棋游戏已经能够有一个比较好的评估函数来综合考虑以上内容。对于围棋而言，使用评估函数的方法似乎没有取得很好的效果，这可能存在两种原因，一种是我们对评估函数的设计有问题，一个错误的评估函数当然得到的评分是错误的。围棋需要考虑的因素要比其它棋类更多，仅仅前面谈到的几点还远远不足以给出一个有效的评估。另外一种可能是评估函数对围棋游戏并不起作用，因为围棋总是存在扭转局势的着法。无论真相是哪种现在看起来似乎都已经不重要了，一种利用利用蒙特卡洛方法进行局面评估的方式比人工编辑特征来得更加有效。

其实围棋的局面评估还有很多别的内容需要考虑，很多细节的东西我们都没有提到，比如死活的判断，是否要打劫等等。说了这么多，也可以看出传统方法在实现上有多么繁琐，可能这也是传统方法无法击败人类的原因吧。围棋游戏看似简单，实则当我们拿着放大镜去看的时候就完全不是我们以为的那个样子了。

## 3.4 蒙特卡洛模拟

### 3.4.1蒙特卡洛算法

蒙特卡洛方法是科学家[冯·诺伊曼](https://baike.baidu.com/item/%E7%BA%A6%E7%BF%B0%C2%B7%E5%86%AF%C2%B7%E8%AF%BA%E4%BE%9D%E6%9B%BC/986797?fromtitle=%E5%86%AF%C2%B7%E8%AF%BA%E4%BC%8A%E6%9B%BC)、[斯塔尼斯拉夫·乌拉姆](https://baike.baidu.com/item/%E6%96%AF%E5%A1%94%E5%B0%BC%E6%96%AF%E6%8B%89%E5%A4%AB%C2%B7%E4%B9%8C%E6%8B%89%E5%A7%86)和[尼古拉斯·梅特罗波利斯](https://baike.baidu.com/item/%E5%B0%BC%E5%8F%A4%E6%8B%89%E6%96%AF%C2%B7%E6%A2%85%E7%89%B9%E7%BD%97%E6%B3%A2%E5%88%A9%E6%96%AF)在[洛斯阿拉莫斯国家实验室](https://baike.baidu.com/item/%E6%B4%9B%E6%96%AF%C2%B7%E9%98%BF%E6%8B%89%E8%8E%AB%E6%96%AF%E5%9B%BD%E5%AE%B6%E8%AF%95%E9%AA%8C%E5%AE%A4/19512540?fromtitle=%E6%B4%9B%E6%96%AF%E9%98%BF%E6%8B%89%E8%8E%AB%E6%96%AF%E5%9B%BD%E5%AE%B6%E5%AE%9E%E9%AA%8C%E5%AE%A4)为核武器计划工作时发明的一种以概率统计理论为指导的数值计算方法。它是一种使用随机数来解决计算问题的统计模拟方法。

很多实际的问题中，我们无法得到精确的数值解，在工程中人们往往可以接受在误差允许范围内的结果。例如虽然我们知道圆的面积公式是：

$$
S=\pi*r^2
$$

但是π是一个无理数，在处理实际问题时我们总是根据现实情况截取小数点后满足需求的位数。比如劳动人民小明想要通过共享经济出租自己房屋的一个圆形淋浴房，淋浴房的半径是45厘米，共享App上规定出租价格必须按淋浴房的实际面积来算，小明的房屋位置靠近CBD区，可以按一平方米20元的价格计算每次的出租价格，他能以什么价格出租呢？

{% hint style="info" %}
根据圆形的面积公式，小明的淋浴室占地0.6361725123519332平方米，按App里的约定，可以每次以12.723450247038663元出租。人民币的最小单位是分，四舍五入，小明每次出租后可以拿到12.72元。以这个价格反向推算出小明的淋浴室面积是0.636平方米，和理论值的误差接近0.027%。
{% endhint %}

如果圆的面积公式从未被发现，将淋浴房设计成圆形的小明是否就无法出租自己的淋浴房呢？非常幸运，蒙特卡洛方法可以帮助小明。通过设计某种统计模拟，就可以近似地计算圆的面积。如果采用统计模拟方法计算得到的精度误差也在0.027%左右，在实际的日常生活中就可以使用这种方法来作为圆形面积的计算方式。

半径为0.45米的圆一定外切一个边长为0.9米的正方形。我们设计一个随机数生成器，它的作用就是随机抽取这个边长为0.9米的正方形中点的位置。我们可以用圆中包含的被抽取的点的数量占正方形中被抽取的点的总数的比例来近似估计圆形面积在正方形面积中的占比。

![&#x56FE; 3-8 &#x8499;&#x7279;&#x5361;&#x6D1B;&#x7B97;&#x6CD5;&#x8BA1;&#x7B97;&#x5706;&#x7684;&#x9762;&#x79EF;](.gitbook/assets/wei-ming-ming-hui-tu-9%20%281%29.svg)

根据算法，我们设计了下面这个小工具，建议读者自行调整其中随机数产生的次数，以观察抽取点的数量对最终结果的影响。

{% code title="MyGo\\tic-tac-toe\\cycle\_area.py" %}
```python
import numpy as np
from numpy import linalg as LA
r=0.45 #半径
side_len=2*r #正方形边长
square_area=side_len**2 #正方形面积
class area_machine:
    def __init__(self,times,r):
        self.times=times #随机生成次数
        self.len=r #圆形的半径
    def isInsideCycle(self,gen):
        return LA.norm(gen,axis=1)<self.len
    def random_generator(self,times):
        return (-1+2*np.random.rand(times,2))*self.len #在正方形中随机生成点
    def run(self):
        dots=self.random_generator(self.times)
        dots_in_cycle=self.isInsideCycle(dots) #查找在圆内的点
        dots_in_cycle_n=np.sum(dots_in_cycle)
        return (dots_in_cycle_n/self.times)*square_area #返回圆的面积

times=[100,1000,10000,100000,1000000,10000000] #请尝试修改这里的数字
np.random.seed(2020)
print([area_machine(i,r).run() for i in times])
```
{% endcode %}

通过小工具的计算，可以得到半径为0.45米的圆面积为：

| 随机次数 | 100 | 1000 | 10000 | 100000 | 1000000 | 10000000 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 圆的面积 | 0.5994 | 0.63828 | 0.639171 | 0.6363117 | 0.63647127 | 0.636210045 |

当随机10000次后，我们得到的面积就已经可以在实际应用中使用了。从结果中也可以看出，随着抽取次数的增加，我们得到的结果和实际值也会越来越接近（0.6361725123519332）。但是也可以看到，随机一百万次得到的结果在精度上反而没有随机十万次高。这是因为电脑产生的随机数属于伪随机数，它们是由数学公式产生的，无法做到真正的随机。但是只要随机次数足够多（比如一千万次），总是可以得到令人满意的精度的。在一些精度要求不高的应用场景下，随机一千次以后得到的值就已经比较准确了（误差0.33%）。我们也观察到，随着抽取次数的增加，蒙特卡洛方法需要更多的时间用于生成模拟数据。如果想要节约时间，可行的一种方案是采用并行计算。但是数值精度和计算资源消耗之间总是会构成一对矛盾。蒙特卡洛方法的优点是简单易用，只需正确地构造概率过程就可以建立目标估计量，但是对于高精度的要求会导致计算资源消耗线性增加，因此这也成为该方法一个为人诟病的缺点。

### 3.4.2蒙特卡洛搜索树

1987年，布鲁斯·艾布拉姆森在他的博士论文中率先探索了蒙特卡洛方法在棋类中的运用。1992年，B·布鲁格曼首次将其应用于对弈智能程序，但当时这种探索并没有受到重视。2006年， 雷米·库洛姆在其2007年的一篇论文中描述了蒙特卡洛方法在游戏树搜索的应用，并将其命名为蒙特卡洛树搜索。之后列文特·科奇什和乔鲍·塞派什瓦里将蒙特卡洛树搜索方法与UCB公式结合，开发了UCT算法。MoGo、Fuego等软件基于UCT算法，都曾在九路围棋中击败了人类业余棋手。

下棋时，棋手需要根据当前局面情况选择对自己最有利的落子点。人类选手会采用类似于Alpha-beta算法的思维策略，蒙特卡洛搜索树方法则采用另外一种策略，通过随机的模拟双方落子，最后汇总找出胜率最高的落子位。通常如果模拟的次数达到一定数量，算法总可以返回一个差不多理想的结果，如果模拟的次数足够多，算法结果会非常逼近实际的最优值。本质上，蒙特卡洛搜索树通过多次实施弱算法，用类似投票的机制对结果进行评价，从而形成一个强效的算法。

![&#x56FE; 3-9 &#x8499;&#x7279;&#x5361;&#x6D1B;&#x641C;&#x7D22;&#x6811;&#x9006;&#x5411;&#x66F4;&#x65B0;&#x4EFF;&#x771F;&#x7ED3;&#x679C;](.gitbook/assets/svg1%20%282%29.svg)

如图3-9，以对弈井字棋为例，当执✖方落子后，执〇方有8个选择，单纯的蒙特卡洛算法不对局面进行评判，为了确认哪个选择更有利，算法随机选择一处〇方落子后开始仿真双方后续的落子情况直到游戏结束，这样就完成了一次随机采样过程。仿真的每一步落子都是随机的，算法每随机仿真一步就会产生一个子树枝节点，每仿真完一局棋局，就会在根节点上产生一条完整的树干。模拟的棋局越多，树枝就会越繁茂。

模拟完一局棋的对弈后算法需要记录仿真的结果，为了使每次仿真的数据能够被最大化利用，对弈的结果不仅要被保留在当前可选的树枝节点上，所有被仿真到的节点都应该记录本节点此次仿真的胜负结果。一种简单的策略是从最末的节点开始更新，顺序更新搜索路径上对应的父节点并反向传播直到更新完当前节点为止。更新节点时可以采用赢一局计一分，输一局减一分，平局不得分的简单记法。

![&#x56FE; 3-10 &#x8499;&#x7279;&#x5361;&#x6D1B;&#x641C;&#x7D22;&#x6811;&#x526A;&#x679D; ](.gitbook/assets/wei-ming-ming-hui-tu-10.svg)

当仿真了足够多的棋局后，蒙特卡洛算法总是选择得分最高的节点作为最优节点的估计。图3-10中，蓝色节点代表游戏对手执✖方实际的落子点，红色节点代表执〇的己方根据蒙特卡洛算法选择的落子点。己方作为后手一开始面对的是对手落子$$X_0$$后的局面，根据算法仿真的结果，己方选择$$O_1$$点作为自己的落子。此时己方可以先维持住当前的搜索树，直到对手下出$$X_1$$的局面。当对手落子$$X_1$$后，与$$O_1$$节点并列的其它兄弟节点就失去了价值，因为他们所代表的落子顺序已不会再在棋局中出现。算法可以将$$X_1$$局面及其下属的枝叶剪出作为一个全新的搜索树来对待，这样做的好处是己方之前在这个局面下做过的仿真计算结果可以保留。如果计算能力足够，也可以把每个局面都当作为一个全新的搜索树来对待，这样做的好处是程序易于实现，但是抛弃了历史的仿真记录，对计算资源的浪费比较严重。有些时候，由于实际环境的限制，模拟的次数可能无法覆盖全部可选项。比如对方下出$$X_2$$的局面，这个情况之前并没有被模拟到过，算法可以选择将其看作一个全新的节点，剪枝出去后抛弃其它所有的历史数据。

我们尝试在井字棋上采用蒙特卡洛搜索树的算法，树的生成使用来[anytree](https://anytree.readthedocs.io/en/latest/)这个库，根据前面的介绍，算法归结起来做了以下几件事情：

{% code title="MyGo\\tic-tac-toe\\ttt.py" %}
```python
class Agent:

    ...

    def chooseMove(self): 

        ...

        if self.mode=='mt': 
                try_times=1000 #1                
                if self.game.last_move is not None: #2    
                        node_tmp=self.tree.findNextNodeByMove(self.tree.tree_root, \
                    self.game.last_move) #3
                        if node_tmp==None: #4
                                self.tree.node_name+=1
                                self.tree.tree_root=Node(str(self.tree.node_name), \
                        parent=self.tree.tree_root,move=self.game.last_move, \
                        loss=0,win=0,draw=0,player=-1*self.player)
                        else:
                                self.tree.tree_root=node_tmp
                node_point=self.tree.tree_root #5
                for i in range(try_times): #6
                        board_=copy.copy(self.game.board.board)    
                        node_start=node_point
                        move_records,game_result=easySimuGame(board_,self.player)
                        for move in move_records:
                                node_start=self.tree.updateLeaf(node_start, \
                        move,game_result) #7
                all_next_leaves=self.tree.getNodeLeaves(node_point)
                all_rates=[(node.loss/(node.loss+node.win+node.draw)) \
                for node in all_next_leaves]
                all_rates=np.array(all_rates)
                pick_move_index=int(random.choice(np.argwhere( \
                all_rates==min(all_rates)))) #8
                self.tree.tree_root=all_next_leaves[pick_move_index] #8
                return all_next_leaves[pick_move_index].move
```
{% endcode %}

1. 设置模拟对弈局数；
2. 判断是不是新开始下棋；
3. 查找当前局面是否在过去的仿真中出现过；
4. 如果第一次见到此局面则将其添加到完整的树结构中；
5. 根据当前局面从搜索树中剪出需要的树枝，抛弃不相关的分支；
6. 采用随机策略多次仿真当前局面下双方的对抗过程；
7. 根据仿真结果更新各节点得分信息；
8. 选择得分最高的节点作为当前局面下的最优估计。

鼓励读者尝试在ttt.py中选择不同的仿真次数来比较AI的对战结果。使用蒙特卡洛的智能程序在每步模拟1000次的情况下，棋力基本上就同采用极小化极大算法的智能程序一样了，而且蒙特卡洛算法在计算耗时上更平滑，初始几步的耗时要明显优于极小化极大算法。

### 3.4.3蒙特卡洛算法改进

蒙特卡洛搜索树对模拟采样的次数要求较高，越多的模拟次数代表需要消耗更多的计算资源，于是我们就需要考虑如何有效地利用每一次的仿真结果。对于博弈游戏而言，很多时候一些落子位是明显没有意义的。以井字棋为例，对于人类选手，一眼就能看出图中用★标注的位置是无论如何不能落子的。而对于单纯的蒙特卡洛算法，由于没有关于游戏的先验知识，算法会把这些标★的位置一并纳入随机采样中，这不仅是对计算资源的浪费，同时也会给游戏的对弈体验带来负面影响，因为AI计算耗费的时间并没有和它的棋力对等。

![&#x56FE; 3-11 &#x7B97;&#x6CD5;&#x4F18;&#x5316;](.gitbook/assets/y1%20%281%29.png)

在围棋这种有很大的分支系数的对弈游戏中，为了提高蒙特卡洛算法的搜索效率，我们可以增加一些辅助算法来限制搜索的范围，辅助算法需要帮助改进的内容包括：

* 不要搜索没有意义的节点，尽可能把计算资源放在搜索价值大的节点；
* 在有限的时间里进行更多的仿真模拟。

![&#x56FE; 3-12 &#x56F4;&#x68CB;&#x7684;&#x6B7B;&#x6D3B;&#x5BF9;&#x6297;](.gitbook/assets/y2.png)

比如在处理围棋的死活时，考虑图3-12的情况。执黑的人类棋手为了做活黑子，对画〇的位置几乎是不会考虑的。另外和被标注上▲的位置相比，人类棋手会更倾向于先思考画✖的位置，因为这些位置从表面上看起来对活棋会更加有利。从模拟人类下棋思考模式的角度出发，我们可以为算法增加一个局面评估函数$$v=F(x)$$，函数的输入是当前棋局，输出是各落子点的潜在价值。函数的目的是辅助蒙特卡洛搜索树算法缩小仿真的范围。

![&#x56FE; 3-13 ](.gitbook/assets/wei-ming-ming-hui-tu-14.svg)

如何得到评估函数$$v=F(x)$$？一般是由专家根据经验手工编辑各种特征规则，函数将输入的局面去匹配这些特征规则后给出评分。如果没有专家，也可以通过机器学习的方法根据大数据样本学习出特征规则。需要注意，评估函数仅仅是起到辅助的作用，它输出的高评分落子位置并不代表这个落子点一定是好的。参考评估函数给出各个落子点的得分，蒙特卡洛算法优先选择得分高的点开始仿真。

很少有人类棋手实战时会下到最后完全没有落子点了才投子结束棋局。大部分围棋的对弈在下到中盘时就已经分出了胜负。采用蒙特卡洛算搜索树算法的智能程序非常依赖仿真的次数，在有限的时间内，我们总是希望可以仿真更多的棋局。自然地，我们希望仿真算法也可以不用下完整盘棋，在差不多中盘的时候就提前评估棋局的输赢。

前面已经介绍过动态局面评估系统，针对围棋，可以采用的一种评估方法是把黑白棋子看作类似正负电荷一样的物质，并将棋盘看作是电子作用下产生的电场。棋子对四周“辐射”自己的影响力，相同的棋子可以叠加自己的影响力力场，不同的棋子则相互抵消各自的影响力。动态评估试图在围棋棋盘上建立自己的物理法则，并以此来确定棋盘上每个棋子的势力范围。在AlphaGo问世以前，很多主流的围棋AI程序都有使用这种思想。

![&#x56FE; 3-14 &#x68CB;&#x5B50;&#x7684;&#x5F71;&#x54CD;&#x529B;&#x8303;&#x56F4;](.gitbook/assets/y4%20%282%29.png)

围棋的目的就是占领比对方更多的棋盘领地。利用动态局面评估机制，我们可以评估当前局面双方的势力范围，计算机程序以此为基础就能粗略地判断棋局的输赢。为了知道什么时候应该截断单次仿真，我们还需要为动态评估系统再建立一个胜负估计函数$$J=F(x)$$。$$F(x)$$输出局面的输赢以及对局面判断的置信度。和评估函数$$v=F(x)$$类似，胜负估计的函数机制也可以人为编制。随着计算机大数据处理能力的增强，采用机器学习的方法获取$$J=F(x)$$会比人为设立规则更轻松简便。

为了得到样本棋局每一回合的胜负置信度，我们引入退化参数R\(0&lt;R&lt;1\)。当样本局结束时，胜负判断的置信度W等于100%。之前的每一回合通过参数R反向传递置信度，可以得到：

$$
R=1/n   (n=当前棋局的回合数)
$$

$$
W(T-1) = W(T)-R
$$

![&#x56FE; 3-15 &#x53CD;&#x5411;&#x4F20;&#x9012;&#x80DC;&#x7387;&#x7F6E;&#x4FE1;&#x5EA6;](.gitbook/assets/wei-ming-ming-hui-tu-17.svg)

可以人为设置一个置信度阀值，当$$J=F(x)$$超过这个阀值时，则提前判定胜负，结束一次仿真。

有了辅助函数$$v=F(x)$$和$$J=F(x)$$，蒙特卡洛搜索树不仅可以控制搜索范围，还可以限制单次仿真的搜索深度，采用这种启发式的方法，算法可以在有限的时间内把计算资源尽可能地用于仿真有价值的落子选项，也就是对最有可能胜利的落子范围进行仿真。如果您读过[金庸](https://baike.baidu.com/item/%E9%87%91%E5%BA%B8)先生的《[天龙八部](http://ds.eywedu.com/jinyong/tlbb/mydoc032.htm)》就会知道，在围棋的世界里，表面看上去占优势的下法未必会赢，而前期劣势的下法可能会在后期转化为巨大的优势，如果只根据评估函数一味地考虑优势下法可能并不是一个好的选择。比如下图这个局面：

![&#x56FE; 3-16 &#x4F18;&#x52BF;&#x4E0E;&#x52A3;&#x52BF;&#x7684;&#x9006;&#x8F6C;](.gitbook/assets/y5.png)

对于白棋而言，评估函数如果偏重考虑做活自己的区域，标〇的位置会得到比较高的评分。虽然白棋在这些区域落子后可以大概率将下半部分棋盘纳入自己的势力范围，但是对于全局而言，黑棋则有机会占领更多的上半部分棋盘，最终由于白棋在下路下的太厚，导致输掉这盘棋。如果白棋在参考评估函数之外能考虑到画✖的位置，破坏黑棋独占上半路棋盘的趋势，则会有更大概率赢得这盘棋。

蒙特卡洛搜索树的改进算法UCT的出现目的就是在树搜索的深度与广度之间找到一个平衡。2006年秋季，两位匈牙利研究人员列文特·科奇什和乔鲍·塞派什瓦里开发了UCT算法，使得围棋智能程序的胜率比当时的最佳算法提高了5%，并且能够在小棋盘的比赛中与人类职业棋手相抗衡。

![&#x56FE; 3-17 &#x641C;&#x7D22;&#x65F6;&#x8003;&#x8651;&#x8BC4;&#x4F30;&#x5EFA;&#x8BAE;&#x4E4B;&#x5916;&#x7684;&#x9009;&#x9879;](.gitbook/assets/wei-ming-ming-hui-tu-9-1.svg)

在评估函数的辅助下，搜索算法总是企图从价值最高的点开始模拟，这会导致算法偏重评估某几个节点，忽略了一些潜在选择项。对于围棋而言，一些落子点的效果很可能要延迟很多回合后才会发挥威力，这些位置的价值在一开始不能通过固定的特征值组合判断出来。所以搜索树在仿真时不能一味地沿着黑色节点前进，并且只考虑评估得分高的灰色节点作为继续的仿真对象。对评估价值低的点也应当偶尔提供计算资源。我们使用UCT公式来平衡搜索深度与广度之间的关系：

$$
v'=ω+c\sqrt{\frac{logN}{n}}
$$

其中$$ω$$是原始的价值函数$$v=F(x)$$的评估结果，$$N$$是本次程序计算时计划仿真的总次数，$$n$$是当前被考虑的节点计划仿真的次数，$$c$$表示人为选择的一个平衡参数。新的价值函数$$v'$$用来代替原始的价值函数，搜索仿真依照新的各节点得分来进行选择。当人为参数$$c$$选的比较大时，评估函数偏重将计算资源投入到原本价值不高的节点上，$$c$$比较小时，仿真则会偏向从评估价值高的节点上开始。

![&#x56FE; 3-18 UCT&#x7B97;&#x6CD5;&#x4E0B;&#x7684;&#x641C;&#x7D22;&#x4F18;&#x5148;&#x7EA7;&#x8C03;&#x6574;](.gitbook/assets/wei-ming-ming-hui-tu-9-5.svg)

根据上图的计算，在使用新的价值函数后，原本被忽略的节点反而被重点考量。超参$$c$$应该如何选择才能达到好的效果可能仁者见仁智者见智。另外注意一点，由于算法调整了原始的评估价值，对应的仿真次数也应当重新分配。比如在总仿真次数固定的情况下，根据评分归一化后均匀分配仿真次数就是可行的方案之一。如果计算资源充裕，采用动态分配也是理想的方法。

蒙特卡洛搜索树算法默认采用随机采样作为缺省的仿真策略。随机策略保证了采样的均匀性，但是在实际应用中，由于博弈规则限制，仿真只能在有限的时间内进行。采用随机策略的围棋AI往往棋力不高，为了进一步提高仿真的有效性，可以考虑使用更复杂的仿真策略，比如策略中可以考虑气 、形 、定式 、攻击模式 、防守模式等一些重要的围棋基本概念。又比如在搜索策略中考虑引入常见棋形的固定下法。人类选手在评估棋力的时候，能记住多少围棋定式也是衡量标准之一。

![&#x56FE; 3-19 &#x9ED1;&#x68CB;&#x6253;&#x5403;&#xFF0C;&#x767D;&#x68CB;&#x957F;&#x7684;&#x56FA;&#x5B9A;&#x4E0B;&#x6CD5;](.gitbook/assets/wei-ming-ming-hui-tu-15.svg)

![&#x56FE; 3-20 &#x56F4;&#x68CB;&#x8D77;&#x624B;&#x603B;&#x662F;&#x4ECE;&#x89D2;&#x4E0A;&#x7684;&#x661F;&#x4F4D;&#x5F00;&#x59CB;](.gitbook/assets/wei-ming-ming-hui-tu-16.svg)

关于实现更复杂的仿真策略的细节部分就不再做过多的展开了，感兴趣的读者可以参考开源的围棋AI引擎[Gnu Go](https://www.gnu.org/software/gnugo/)、[Fuego](http://fuego.sourceforge.net/)和[Pachi](http://pachi.or.cz/)。GnuGo的搜索算法是众多围棋软件中最强的，它使用了更加优化的搜索算法，棋力超过了普通采用蒙特卡洛搜索树的程序。而Fuego和Pachi则是采用了更加先进的优化方案，棋力上是比Gnu Go更胜一筹。

### 3.4.4需要注意的问题

对一个可能有希望的落子点随机仿真10次后有7次赢得棋局，我们有多大的信心认为这个落子点是一步好棋呢？答案是：并不乐观。假使实际上这个落子点对胜负的影响是平衡的（胜负各50%的概率），通过随机的方法我们也会有30%的机会得到7胜3负的结果。但是如果是100局仿真中有70次胜利呢？这种情况下，我们可以很自信地相信当前仿真的落子点是一步好棋。由于围棋的分支实在太多，对当前局面的下一步棋来说，一般需要仿真近千次才会有一些自信判断是否是一步好棋。通常留给计算AI程序思考的时间有限，而在一回合内至少需要仿真一万次以上才能基本上满足算法给出最优判断，此种情况下程序实现需要尽可能的优化，如果仿真一局都要耗时好几秒钟，即使算法再好，其实用性也会大打折扣。从提升性能的角度出发，仿真程序在每一回合都要对所有可选分支进行价值评估也是一笔不小的计算开支。如果仿真策略不使用随机策略，而是采用人工编制的高级策略，可以考虑不用每一步都计算各节点的评价得分，对于自信度高的下法可以跳过计算评估的动作。但是无论如何优化和改进，UCT算法只能在分支少的小棋盘上赶上人类选手，在19路围棋棋盘上依然还是只能达到初级选手的水平。

## 3.4 监督学习

监督式学习是机器学习的一种方法，它可以让系统由训练数据中学到或建立一个模式，并依此模式推测新的实例。训练数据包括一套训练实例集，其中每个实例都是由一个输入的样本和一个预期输出标签所组成。监督学习算法就是分析该训练数据实例集，并使得系统可以逐步拟合出样本和标签之间的映射关系。如果把系统用函数来表示，这个函数的输出可以是一个连续的值，或是预测一个分类标签。通俗地来讲，监督学习就像是老师指导学生学习知识。老师负责出题并告诉学生给出答案是对是错。学生在学习过程中借助老师的提示获得经验，逐渐调整自己的认知，最后对没有学习过的问题也可以做出正确的解答。

在监督学习中，我们只需要给定输入样本集，机器就可以从中推演出指定目标变量的可能结果。机器只需从输入数据中预测合适的模型，并从中计算出目标变量的结果。监督学习要实现的目标是“对于输入数据X能预测变量Y”。几乎所有的回归算法和分类算法都属于监督学习。传统上属于监督学习的算法有：K-近邻算法、决策树、朴素贝叶斯和逻辑回归。

目前神经网络已经成为最为热门的监督学习算法，本书后面的章节中也将会使用神经网络作为基础工具来实现超越人类水平的围棋智能程序。在围棋这个主题上，单纯地使用神经网络很难在棋力上取得突破。这可能和围棋本身的复杂程度有关。关于使用神经网络来进行监督学习的内容将放在下一章进行详细的介绍。在那之后我们还将通过传统的监督学习来实现一个具备一定棋力的神经网络。

## 3.5传统方法的讨论

在国际象棋更加流行的西方，人工智能的研究在开创之初就有人考虑如何制作一个能够下国际象棋的机器。20世纪80年代初，贝尔实验室的工程师们开发出了历史上第一个具有人类大师级水平的国际象棋机器“Belle”。Belle 由三个主要部分组成：移动生成器，评估器和变换表。移动生成器负责识别出遭受攻击的最高价值部分和最低价值部分，并根据该信息对潜在的移动行为进行排序。评估器会注意到“王”在比赛的不同阶段的位置及其相对安全性。变换表包含了潜在可移动的选项数据缓存，它可以使评估更加有效。Belle 采用了暴力的方法。它查看了玩家当前配置的棋盘可能做出的所有动作，然后又考虑了对手可以做出的所有动作。在国际象棋中，一名玩家移动一个棋子称为一层。最初，Belle 可以计算 4 层深度的移动。当 Belle 于 1978 年在计算机协会北美计算机国际象棋锦标赛上首次亮相时，它的搜索深度为 8 层，并夺得了冠军。

在 Belle 统治计算机国际象棋世界多年之后，它的明星效应开始褪色。80年代末，卡内基梅隆大学的[许峰雄](https://baike.baidu.com/item/%E8%AE%B8%E5%B3%B0%E9%9B%84)博士在 “Bella”的思路基础上进一步改进，研制出了第一个特级大师水平的国际象棋机器，取名“深思”。随后，许博士加入IBM研究院，在那里他和其他几个团队成员一起研制出了实力更强的弈棋机器“深蓝”，并最终于1997年的一场历史性的人机大战中以3.5：2.5的比分战胜了人类国际象棋冠军卡斯帕罗夫。深蓝并没有对传统的暴力算法进行太多的改进，客观地来说，深蓝不是基于AI技术构建的，它是一组专门为国际象棋而设计的CPU集群。深蓝也有棋类游戏智能软件常见的配置：开局库、着法生成器、评价函数、剪枝算法等。为了追求极致的搜索速度，它没有通过软件方式来实现，而是依靠专门为国际象棋设计的计算芯片，通过每秒计算两亿步的恐怖算能把人类甩了在后面。插句题外话，《[“深蓝”揭秘](https://book.douban.com/subject/1491268/)》这本书揭秘了深蓝这台价值数百万美元的超级计算机背后的故事，许峰雄博士将自己的亲身经历以传记的形势呈现在大家面前，非常有意思，读者如果感兴趣可以尝试阅读一下。

2000年开始，随着计算机硬件技术的发展和研究者对Alpha-beta算法进一步的研究和优化，越来越多的棋类对弈软件开始逐步超越人类专业棋手。国际象棋的智能软件已经不再依赖专用象棋芯片就可以轻易战胜人类特级大师。国人制作的中国象棋软件也同样已经达到了无人匹敌的地步。这些弈棋计算机AI背后的基本“思考模式”都很简单，就是对当前局面下的每一种合法走法所直接导致的局面进行评估，然后选择“获胜概率”最高的局面所对应的那个走法。可以说计算机的基本策略是所有“人类有可能采用”的策略中最原始最简单的一种。这些弈棋AI只关心 一个问题，就是按照“胜”的基本定义来赢得比赛。“在当前局面下，我走哪一步能赢？”，计算机就是通过不停地重复问自己这个问题来完成对弈的。

对计算机而言，从给定盘面开始的局势变化的复杂度是随考虑的步数呈指数级增长的。对于包括围棋和象棋等绝大多数复杂棋类运动而言，这就意味着从原则上不存在能够准确计算盘面最优结果的有效方法。文献表明，19路标准围棋棋盘的搜索空间是中国象棋的10210倍，是国际象棋的10227倍。 普通人很难想象这个数字背后代表的物理差距，一个直观的理解是：原子的大小大约是10-15m，而太阳引力所能影响到的空间范围大约是原子核体积的1090倍，也就是说围棋相比与象棋的复杂度，比整个太阳系相对与一个原子的比例还要大。总之，除了人工设计的博弈项目之外，围棋是人类历史发展过程中所产生的最复杂的博弈项目，并且其复杂程度远远超过其它博弈项目。 

下表列出了常见棋类的状态空间复杂度和博弈树复杂度。状态空间复杂度是指从博弈初始状态开始所能达到的所有不同合法博弈状态的个数。它描述了通过枚举所能解决的博弈复杂度范围。通常情况下，准确地计算博弈的状态空间复杂度是很困难的，也是不必要的，一般我们都会对其进行一个估算，有大致感性的理解即可。博弈树复杂度指的是从博弈初始状态所产生的、能完整解决该博弈问题的、最小博弈树子节点的个数。博弈树复杂度描述了通过极小极大搜索所能解决的博弈复杂度。准确计算围棋博弈树复杂度也是几乎不可能的。我们只能粗略的估算。实际在计算机AI搜索策略时，需要面对的是博弈树展开后的复杂度。

| 游戏名称 | 状态空间复杂度 | 博弈树复杂度 |
| :--- | :--- | :--- |
| 西洋跳棋 | $$10^{21}$$  | $$10^{31}$$  |
| 国际象棋 | $$10^{64}$$  | $$10^{123}$$  |
| 中国象棋 | $$10^{48}$$  | $$10^{150}$$  |
| 围棋 | $$10^{172}$$  | $$10^{360}$$  |
| 黑白棋 | $$10^{28}$$ | $$10^{58}$$  |
| 将棋 | $$10^{71}$$  | $$10^{226}$$  |

可以看出，跳棋这种规则简单且步数有限的棋类游戏，它的合法状态空间复杂度都有10的21次方之巨。对于弈棋AI的设计者来说，“不可能对局势变化的所有可能性进行有效计算”意味着想做得比对手更好需要从原理上解决两个关键问题： \(1\)决定一个“筛选策略”，从“所有当前盘面出发有可能导致的变化”中选择一部分作为“我们实际考虑的那些局面变化”；\(2\)决定一个“汇总策略”，把所有实际考虑的变化的静态评估结果综合起来，对当前盘面的胜率完成评估。

归根结底，传统算法的目的就是尽可能正确地”打分“和尽可能多地”穷举“。无论是国际象棋、中国象棋、围棋、或者西洋跳棋、黑白棋等等，这些棋类程序在AlphaGo出现以前，都围绕着这两点来进行基本框架的搭建。它们之间的区别仅在于使用的具体策略不同，以及针对不同的策略采用的不同优化手段。传统方法在搜索复杂度不是特别高的情况下可以工作的很好。比如中国象棋和国际象棋的AI程序早在十几年前就已经赶超了人类最顶尖的职业选手。不知道为什么，同样的策略作用在围棋软件上效果却泛善可陈。也许是围棋规则太过简单，又或者是在下围棋的时候并不是时时刻刻都要秉承着其最终胜利的原则。 汉代扬雄在其《法言·君子》中提到：“昔乎颜渊以退为进，天下鲜俪焉”，意思是“以谦让取得德行的进步，以退让的姿态作为进取的手段”。传统方法时时刻刻都在基于“胜”的定义进行思考可能太过激进了。而AlphaGo背后的强化学习策略一改传统的设计思路。 它不是一开始就奔着设计出一个强大的AI程序为目的，强化学习只是教给了AI程序基本的游戏规则，然后让程序自己在游戏中学习， 这种方法目前看起来更为贴近人类的学习路径，而达到的效果也更好。

