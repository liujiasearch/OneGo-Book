# AlphaZero

AlphaGo解决了如何更有效地利用蒙特卡洛搜索树来逼近围棋的最优解，并证明了利用蒙特卡洛搜索树来逼近围棋游戏的最优解是完全具有可行性的。既然蒙特卡洛方法是有效地，那么一个自然而然能想到的问题就摆在了眼前，与其让神经网络学习如何下围棋，干嘛不直接用神经网络来学习这棵搜索树呢。AlphaZero就这么诞生了。

从最初的随机落子到目前我们学习的AlphaGo，我们设计的AI智能体都只会不停的下棋，不管落子是不是必须的，或者说有意义的。这是由于之前的算法中我们没有可以教会AI虚着的样本。人类选手不会在围棋结束前虚着。没有可以参考的样例，我们也就没有办法通过监督学习使计算机来学习可以虚着的棋盘特征。

AlphaZero克服了向人类棋手学习的过程，它直接从零开始，除了外部设置的游戏规则，不需要借助人类的其它先验知识，通过自己跟自己下棋，AlphaZero可以不断地提升自己的棋力。在这个过程中，我们允许AlphaZero使用虚着，它从一开始下棋的时候，虚着和实着就是同时存在的。

我们使用AC结构的网络来提升AlphaZero的学习效率。AlphaZero没有采用类似AlphaGo使用简单网络快速落子的方式。AlphaGo需要三个神经网络，而AlphaZero仅使用AC结构的神经网络，在蒙特卡洛仿真的整个过程中都使用这个网络。

由于AlphaZero需要学会虚着，面对19路棋盘时，网络在策略输出时的一维数组长度不再是361位，而是361+1位，最末一位输出代表虚着。AlphaZero把当前棋盘局面作为根节点，并以此为基础展开博弈树，一次整个的仿真过程描述如下：

1. 从根节点出发，当前节点表示的棋局是否已经结束，如果结束了，就跳到第6步；
2. 当前节点是否存在子节点，如果没有子节点，就计算AC网络的策略输出，并初始化所有输出子节点的Q，P，N和n值。Q表示棋局仿真得到的价值，初始由于还没有开始仿真，设置为0。P表示AC网络的策略输出值。N表示父节点被访问的次数，n表示当前节点被访问到的次数，n初始时为零，子节点的N就是父节点的n；
3. AlphaZero也是基于蒙特卡洛方法，它也采用UCT公式作为博弈树分支选择的依据；
4. 更新当前通过UCT公式选择的节点的n值；
5. 重复第1步直到一局仿真结束；
6. 评估棋局的胜负，并逆向更新Q值。

第3步的UCT公式如下：

$$
T=Q+cP\sqrt \frac{N}{1+n}
$$

和几乎所有的UCT公式一样，我们需要c这个超参来平衡AC网络的输出与仿真结果对最终T值的影响。

第6步更新Q值的公式如下：

$$
Q=\frac{\sum_{i=1}^n V_i}{n}
$$

其中V表示每一次仿真的结果，我们用1表示己方胜利，-1表示对方胜利，如果在限定的深度内没有结束棋局则用0表示平局。在模拟对方落子时，对己方而言Q值和P值都是负数，所以取模拟的对方T值时要取最小值，而不是最大值。

![AlphaZero](.gitbook/assets/alphago_zero-1-.svg)

在有限的时间内我们希望仿真的局数越多越好。当多次仿真完成后，和所有的UCT算法一样，我们选择根节点下被访问次数最多的节点作为最终智能体的着法输出。

有了智能AI的下法，有了AC网络，接下去要解决的问题是如何训练这个AC网络。根据之前的讲解我们知道，强化学习最终也是需要通过监督训练来实现智能体水平的提高，强化学习仅仅是通过算法从而实现自己提供自己学习的样本。由于我们希望AlphaZero学习的不是围棋的着法，而是蒙特卡洛博弈树的仿真结果，因此当智能体完成一步落子前的仿真后，我们提取的样本不是最终的着法，而是根节点下一层的所有节点的被访问次数。以前其它算法提取的样本都是当前局面与着法的对应关系，而AlphaZero则是提取当前局面与子节点上被访问的n值的关系。另外，AC网络的输出节点使用了softmax激活函数，每个节点的输出范围在0到1之间，因此我们还需要把样本标签进行归一化处理。 剩下的一切内容就和AC网络的训练和学习过程一模一样了。

![](.gitbook/assets/gozeroquanzhong.svg)

由于我们允许智能体AI下出虚着，仿真过程中，如果博弈树连续两次出现虚着不应该结束棋局。另外对于仿真出现连续两次虚着而棋局并没有结束的情况，需要做一下特殊处理。



