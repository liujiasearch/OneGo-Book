# temp



需要注意一点，调用import keras时，如果本地没有数据时，数据不会下载，只有在实际赋值使用时才会

该选择结构可以灵活对theano和tensorflow两种backend生成对应格式的训练数据格式。举例说明：'th'模式，即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。而TensorFlow，即'tf'模式的表达形式是（100,16,32,3），即把通道维放在了最后。这两个表达方法本质上没有什么区别。

策略梯度 就是根据结果对已经下过的棋进行取舍 q-learning 就是学习输入棋局，合法步预测出输赢价值 上面两种的本质都差不多，就是根据结果对监督学习的学习目标进行归类（好的与坏的），然后再学习





辐射动态评估会考虑渗透度P，衰减度A和方向阻尼D。渗透度是为了解决在辐射方向上受到阻拦时的表现。一般在没有遇到有效阻拦时候，这个值为1，遇到有效阻拦时，这个值为0。衰减度是指源对外影响衰减的比例，这个值在不同方向上有所不同，比如对角线上的衰减程度是普通的两倍，修改衰减度可以调整源的影响范围。方向阻尼模拟了波在传播过程中遇到阻碍的情况，这里用作计算辐射作用绕过干扰棋子，继续在该方向上作用的衰减度。一般可以用传播方向与需要计算的那个位子与源的朝向之间的夹角余弦的平方。

其实，围棋的静态评估还有很多别的要考虑，很多细节的东西我们都没有提到，比如死活的判断，是否要打劫等等。说了这么多，也可以看出传统方法在实现上有多么繁琐，可能这也是传统方法无法有效击败人类的原因吧，围棋游戏看似简单，实则当我们拿着放大镜去看的时候就完全不是那个样子了。



