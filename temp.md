# temp



需要注意一点，调用import keras时，如果本地没有数据时，数据不会下载，只有在实际赋值使用时才会

该选择结构可以灵活对theano和tensorflow两种backend生成对应格式的训练数据格式。举例说明：'th'模式，即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。而TensorFlow，即'tf'模式的表达形式是（100,16,32,3），即把通道维放在了最后。这两个表达方法本质上没有什么区别。

策略梯度 就是根据结果对已经下过的棋进行取舍 q-learning 就是学习输入棋局，合法步预测出输赢价值 上面两种的本质都差不多，就是根据结果对监督学习的学习目标进行归类（好的与坏的），然后再学习



围棋传统方法非常复杂，要考虑的要素也相当多，这全都要怪算法中引入了太多人类的主观意见。但是alphago所代表的方法则要简单很多，简单的反而战胜了复杂的。这就说明技术在进步。回望过去，我们可以发现，过去的很多东西在工程上实现起来很费时费力，但是现在就要简单许多了，这不仅仅体现在计算机编程上，其它领域也是如此，我们可以如此深切的体会到时代与技术的发展。同时，我们也可以提炼出一条信念，如果时代进步了，如果社会进步了，那么生活就应该变得简单，如果没有，那所谓的进步就是谎言。

辐射动态评估会考虑渗透度P，衰减度A和方向阻尼D。渗透度是为了解决在辐射方向上受到阻拦时的表现。一般在没有遇到有效阻拦时候，这个值为1，遇到有效阻拦时，这个值为0。衰减度是指源对外影响衰减的比例，这个值在不同方向上有所不同，比如对角线上的衰减程度是普通的两倍，修改衰减度可以调整源的影响范围。方向阻尼模拟了波在传播过程中遇到阻碍的情况，这里用作计算辐射作用绕过干扰棋子，继续在该方向上作用的衰减度。一般可以用传播方向与需要计算的那个位子与源的朝向之间的夹角余弦的平方。

其实，围棋的静态评估还有很多别的要考虑，很多细节的东西我们都没有提到，比如死活的判断，是否要打劫等等。说了这么多，也可以看出传统方法在实现上有多么繁琐，可能这也是传统方法无法有效击败人类的原因吧，围棋游戏看似简单，实则当我们拿着放大镜去看的时候就完全不是那个样子了。



