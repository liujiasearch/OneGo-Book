# DQN

这一章节，我会使用Pytorch来实现神经网络的机器学习。之所以换个框架工具，一是由于从代码上，上一章学到的keras已经足够使用了，读者完全可以自己轻松地使用Keras来实现本章的内容。二是由于Pytorch的使用越来越广泛，特别是学术界。也许某一天Pytorch会超过Tensorflow，成为业界首选。三是我的旧电脑已经没有办法使用TensorFlow1.8以上的GPU版本了，即便我使用了非官方编译的版本，但是我的AMD X2 550芯片在非官方版本下有时还会报一些奇怪的错误。Pytorch则似乎运行的很好，当我的新电脑在没日没夜劳作的时候，我还可以使用我的老旧电脑运行一些其它的深度学习程序。4是我觉得多学一点东西对读者总是好的，即使读者想一直使用Keras，开拓一下眼界也无伤大雅。虽然我不得不承认第三点是导致我采用Pytorch来演示本章最主要的原因。不过当我使用了Pytorch以后，我觉得它比TensorFlow更方便一些，如果没有Keras这个高级API，我会选择使用Pytorch来实现原型搭建，而如果我有自己想实现的特别算法，并且没有现成keras模型可以使用时，我也会选择Pytorch。当然，对初学者而言，Keras作为TensorFlow的高级API相比Pytorch会更加友善一些。

我们在策略梯度里提到了强化学习的学习标签不再是概率的概念，而是引入了价值的概念。但是策略梯度算法的核心并不是价值驱动的，它还是有概率的影子在里面，比如我们在智能体互弈的过程中没有按照策略提供的最大价值来采取行动，而是把价值看做是采取不同行动可能的概率来对待。这一章我们使用Q-Learning的算法，以价值为驱动来指导智能体下围棋。

## 传统的Q-Learning算法

### 原始版Q-Learning

我们通过一个实例来引入Q-Learning的介绍。假设我们有7个小格子，在小格子的最右边尽头有一面旗帜，第二个小格子上有一个小人，每次我们可以向左或向右移动小人一格，如果小人走到了旗帜上游戏就胜利，否则就要一直移动下去。这个游戏对人类来说很简单，我们从感官上就能知道只需要不停地把小人向右移动就可以了，但是我们的智能程序就没这么智能了。它对外界的情况几乎一无所知，只知道小人可以左右移动以及拿到旗帜就能得到奖励这个目标。

![](.gitbook/assets/xiao-you-xi-.svg)

谈到Q-Learning，一个躲避不开的事情就是奖励机制。为了鼓励智能程序控制小人能够拿到旗帜，我们把移动小人得到旗帜这件事情奖励给智能程序1分，如果移动小人后什么也没有发生就不得分。我们的智能程序在最开始的时候其实并没有什么智能，所以它能做的只是随机的左右移动。这个游戏本质上就是一个一维的随机漫步，根据随机游走的理论，只要我们的智能程序随机的左右移动小人，就一定能在某个时刻拿到旗帜。拿到旗帜后，我们的智能程序显然会发现了一个事实：当小人处在编号为6的格子上，只要向右移动就能得到奖励1。于是智能程序就在自己的小本本上记了这么一笔：6走右得1。之后只要走到6这个格子上，我们的智能程序就一定让小人往右走，因为一切都记录在了它的小本本上。随后它又发现，如果小人在5这个格子，只要往右走，就能走到格子6，而走在格子6上下一步就会赢，于是它又在自己的小本本上记了一笔：5走右得1。显然要不了多久我们的智能程序就会知道从起点开始，只要不停地往右移动小人就能拿到旗帜。再看一下我们只能程序的小本本，它记录了如下内容：

| 格子 | 往右 | 往左 |
| :--- | :--- | :--- |
| 1 | 0 | 0 |
| 2 | 1 | 0 |
| 3 | 1 | 0 |
| 4 | 1 | 0 |
| 5 | 1 | 0 |
| 6 | 1 | 0 |

上表就是Q-Learning的策略表。一开始它的所有记录都是0，随着随机仿真获得了或正或负的价值，这张表就会被逐渐填上适当的值。策略表记录的内容就是在各个不同的情境下智能体做出各种不同动作能得到的相应的价值。

完整Q-Learning算法会比刚才说的简化版本还要再复杂一些。首先我要引入“现实的价值”与“估计的价值”两个概念。所谓现实的价值就是指我们的智能体在游戏中操作小人，每操作一步后，这个游戏反馈回来的价值。第一轮小人不管怎么移动，只要没有走到第7格，游戏反馈的价值都会是零。当第一轮游戏结束后，我们会得到关于第六格的信息：

| 格子 | 往右 | 往左 |
| :--- | :--- | :--- |
| 6 | 1 | 0 |

这个在第六格往右得到的1，就是现实的价值，是游戏反馈我们的。所谓估计的价值，指得是智能体对采取的行动可能得到的后果的一个估计。比如从第二轮开始，当智能体移动到第六格时，它查了一下自己的小本本，发现往右走能得到1分，所以它就会对自己在第六格走出下一步可能获得的价值有一个期待值，即1。给出“现实价值”的主体是我们的这个游戏，而给出“估计价值”的主体是我们的智能体。

细心的读者可能已经发现了一个问题，现实的价值似乎不是时时都有的，游戏可能并不会对智能体的每个行为都给出反馈，这在其它游戏也是一样。打飞机游戏不可能每操作一次摇杆都会打落一架敌机。即便我们的生活也不是每付出一次努力都能得到相应的回报。于是为了高效地更新现实价值，我们引入“折现”的概念。在策略梯度中有提到过，折现的概念来源于经济学，它是很多金融产品定价的基础。我们这里不谈它的金融意义，仅借用其数学含义，来提高学习的效率。我们把现实的价值定义为“即时的回报加上未来回报的折现”。只要游戏没有结束，未来的回报总是存在的。谈到折现，就会引入折现率，在金融的背景下，折现率等于：

$$
P=\frac{1}{(1+r)^n}
$$

其中r表示一次计量间隔单位的利率，一般以年为单位。n表示往前折现多久，n=1表示下一年的金额在当前的价值。假设年利率是3%，两年后我凭手上的国债券可以到银行兑换11000元现金，那么这张国债券现在卖出可以值多少呢？根据刚才的公式，我们把r=0.03和n=2带入公式，得到P=0.9426。将11000乘以折现率P，得到现值10368.56，即当前我手上的国债券可以以10368.56的价格出让。换一种说法，读者可以认为如果我现在手上有10368.56元现金，我把它存在银行2年，银行给我的年利率是3%，两年后我从银行连本带息地取出时，一共可以拿到11000元现金。在Q-Learning的算法里，我们借用了折现率的概念，目的是类似的，我想知道未来状态能获取的价值在目前状态的价值。通常我们直接设P=0.9或者0.99这样，在Q-Learning里不用考虑利率r，后面的阐述中，假定P=0.9，这也是一个超参，读者可以尝试为其赋予不同的值。

回到游戏中，当第一轮游戏结束后，我们知道第六格能够预期得到的最大价值等于1。假设在二轮游戏时智能体已经把小人移动到了第5格上，它看了一下自己的小本本，除了第六格有记录，其它格子的记录都是0，于是智能体随机地把小人向右移动了一格。移动后小人到了第六格，由于没有到达终点第七格，游戏给出的即时回报是零，但是小本本上记录着第六格的下一步能得到的最大价值是1，这就是未来的回报，再引入我们折现的概念，这样我们得到未来第7格的回报折现到第六格的奖励值等于0.9。于是综合现实价值的定义：

$$
现实价值 = 即时的回报+未来回报的折现
$$

就有了我们从第五格走到第六格能够获取的全部现实价值等于0+0.9=0.9。

再来看一下期望价值，智能体查了一下小本本关于第五格的记录，上面记录如下：

| 格子 | 往右 | 往左 |
| :--- | :--- | :--- |
| 5 | 0 | 0 |

当智能体最终选择往右走的时候，它对未来获取价值的估计是零。

现实的价值是客观存在的，是外部环境对智能体的行为给出的反馈，我们无法改变现实的价值，只能不断地优化自己估计价值的能力。不过需要注意，任何改变都不应一蹴而就，循序渐进方是上策。这个道理拿到我们现在的Q-Learning学习也是一样，在更新估计值的时候，我们不当一次性就拿现实值去替换估计值，道理很简单，在复杂场景下，现实值也是会动态变化的，我们获取到的一次现实值并不能真实反应事实的全部面貌，因此我们宁愿多尝试几次，每次只学一点点。于是和所有的机器学习一样，我们需要一个学习率参数，一般可以设置参数等于0.1或者0.01，这也是一个超参。有了现实值，有了旧的估计值，新的估计值也就呼之欲出了。新的估计值等于旧的估计值加上学习率乘以旧的估计值与实际值之间的差异。用文字描述真的很费劲，用公式来解释就显得直观许多了：

$$
Q_{s,a}^R\\'=R_{s+1}+P*max(Q_{s+1,a'}^p)\quad\quad\quad\quad(1)\\
Q_{s,a}^P\\'=Q_{s,a}^P+L*(Q_{s,a}^P-Q_{s,a}^R\\')\quad \ \ \ \quad\quad\quad(2) \\ \ \\ 
其中Q_{s,a}^R\\'表示在状态s时选择行动a得到的现实价值   
\\Q_{s,a}^P表示在状态s时选择行动a的预测价值
$$

公式中，旧的估计值只要查一下智能体的小本本就能知道。Q-Learning算法就是不断迭代上面的公式\(1\)和\(2\)并更新智能体自己的策略小本本，用不了几轮迭代，我们的智能体就能掌握迅速完成这个小游戏的窍门了。

{% code title="myGO/q-learning.py" %}
```python
def dl(state,newState,action,reward):
    q_predict=vTable.loc[state,action]    #1
    if states.index(newState) != numState-1:
        q_real=reward+dis_r*vTable.loc[newState].max()    #2
    else:    #3
        q_real=reward    #3
    vTable.loc[state,action]=q_predict+lr*(q_real-q_predict)    #4
```
{% endcode %}

1. 在状态s并使用行动a后通过查表获取旧的估计价值，旧得估计价值本质上就是智能体对采取行动a后所能获取的现实价值的最佳估计；
2. 采用行动a后，外界反馈了现实价值reward。为了更加高效地更新现实价值，我们同时对下一状态的最佳估计进行折现。这一步对应前面的公式\(1\)；
3. 在游戏结束时，只有外界奖励的现实价值。不再存在未来，也就没有未来现实价值的折现了；
4. 根据实际情况和预计情况的对比，更新状态s采取行动a后的估计值，并替换原来的旧估计值。这一步对应前面的公式\(2\)。

在Q-Learning的学习过程中，由于外界的反馈是不确定的，在智能体真正地实施行为后才能得到外部的反馈，于是智能体只能根据自己的小本本来选择合适的动作行为。但是为了避免陷入局部最优，在学习时，我们需要为智能体引入一定比例的随机行为。即使智能体的小本本上说往右走能得到好的结果，但是一旦满足随机条件，我们就让智能体不遵照小本本的指导，随性地做出一个不负责任的选择。通常这个比例是1%，不过也可以动态地设置这个比例，可以一开始比较高，随着学习获取的知识逐渐增多，这个比例就慢慢地下降到零。

{% code title="myGO/q-learning.py" %}
```python
def chooseAction(state):
    policy=vTable.loc[state]
    if np.random.rand()<=epsilon or 
        policy.all()==0 or policy.left==policy.right :    #1
        action=np.random.choice(actionG)    #1
    else:
        action=policy.idxmax()    #2
    return action
```
{% endcode %}

1. 当策略中没有显著建议或者满足随机比例时，无论智能体处于什么状态，都随机的执行一步行动；
2. 如果没有满足随机选择的条件，就取状态s下能带来最好估计价值的行为。

### 原始版Q-Learning计算时的优化

在游戏的一开始，我们的Q表里没有任何东西（全是0），我们的智能体对外界环境一无所知，它只能不停的采用随机尝试，期盼着突然在某个时刻得到上帝或者菩萨的悲悯，让它得到游戏的奖励，结束这看似无尽又痛苦的尝试。对我们上述那个小游戏而言，这种慈悲是很快就能得到的，因为它实在是简单到令人发指。可是如果这样一个迷宫呢：

![](.gitbook/assets/mi-gong-.svg)

采用随机的方法，从起点出发，粗略地算了一下（不精准），找到终点的概率大概只有1.19861E-06，翻译成人类语言的意思就是说需要尝试将近834300次才能由起点走到终点，这还是最理想的情况。虽然834300这个数字对现代计算机而言不是什么大不了的事情，但是我们的原始版Q-Learning一次只能更新一个状态，在第一轮游戏时，由于大部分情况小人是没有得到过任何奖励的，所以现实的奖励值。同理对于估计值来说也一样是几乎是没有任何更新了。

Q-Learning的更新趋势是从奖励点（终点）开始往起点方向逐步更新，如果我们把起点设置在紧挨着终点的左边呢（图示×的位置）？显然这时候有25%的概率可以立即走到终点。我们不需要尝试834300次，而是有25%的可能1次就得到奖励。之后我们再把起点放置于靠近×的地方（图示〇的位置），显然如果从〇出发，由于Q表已经有了×点的记录，只需走到×点就能知道走到终点得到奖励的走法，我们以此往复逐点更新Q表，知道更新到起点处的Q表对应的值。用这种方法将大大减少我们使用随机策略尝试的次数，除了开头的几步可能走到无记录的位置上，一旦走到Q表存在记录的位置，就能迅速找到走到终点夺取奖励的方法。而且随着更新的深入，Q表上无记录的位置将会越来越少，我们尝试的时间和次数也是会越来越少。

![](.gitbook/assets/mi-gong-2.svg)

不过有些时候，我们即使有能力可以虚拟出游戏环境，但是我们可能并不知道“起点”应该在哪里。比如围棋游戏，我们甚至连终点应该长什么样子都不知道。另外手工编辑各个场景的意义也不大，现实中我们需要更加普适的方法。一种可行的方式是我们随机地创建场景，具体到上面这个例子，我们就不再控制×点或者〇点的位置了，而是采用随机的方法，将起始点随机地落在整个迷宫里。最差情况下这种方法可能依然摆脱不了大量无用的尝试动作，但是从数学期望上来说，至少可以将随机尝试的次数减少一半，而且第一次随机就随机到最差情况的可能性不来就不高。

{% code title="myGO/q-learning.py" %}
```python
for episode in range(episodes):
    step=1
    isOver=False
    #state=states[np.random.randint(numState-1)]    #1
    while not isOver:
        #state=states[np.random.randint(numState-1)]    #2
        action=chooseAction(state)
        state_,reward=env_resp(state,action)
        dl(state,state_,action,reward)    #3
        state=state_
        if states.index(state) == numState-1:
            isOver=True
        step+=1
        showS(state,numState,episode,step)
```
{% endcode %}

1. 随机的方式一是每一轮完整的游戏都从一个随机的状态开始；
2. 随机的学习方式二是每一步都从一个随机的状态开始；
3. 有一点没有在前面说明，更新的动作是智能体每做一次行动都要有的。虽然从表面上看，真正有数字变化的更新是在获取实际价值以后，但是实际上只要游戏没有结束更新的动作是从来没有间断过的。随着学习的次数越来越多，即使当前行为不会立刻带来实际的奖励价值，但是可以把未来预期的价值逐渐折现到当前的状态下，使得智能体可以沿着折现值的轨迹更快地找到最优解。

根据经验来说，随机方式一更具有普适性，但是方式二学习的效率更高，更快。具体使用哪一种优化方式，或者由于条件限制可能无法使用优化方案，这些都要根据实际情况来判断，不同的问题可能有不同的处理方案，条件允许的情况下，建议总是采用优化方案以加快学习的速度。

### Q-Learning的变种Sarsa

读者如果自己尝试使用一下Q-learnning的话，可能会发现一个问题：我们在折现未来的预期收益时使用的是未来能预见的最大收益，但是智能体在进入下一状态后有一定概率并不按照小本本上记录的最大收益来行动，而是采取了随机的一步行动，导致实际行为和估计算法出现了不一致。Sarsa则是调整了这里的不一致，它的估计总是更实际行动相匹配的。我们稍微调整一下Q-Learning现实价值的公式\(1\)就能得到Sarsa的算法：

$$
Q_{s,a}^R\\'=R_{s+1}+P*Q_{s+1,a'}^p\quad\quad\quad\quad(1)\\
Q_{s,a}^P\\'=Q_{s,a}^P+L*(Q_{s,a}^P-Q_{s,a}^R\\')\quad \ \ \ \quad\quad\quad(2) \\ \ \\
$$

{% code title="myGO/q-learning\_sarsa.py" %}
```python
def dl(self,action,location,newLocation,how,env,isOver,reward):
    if how=="Q":    #1
        ...
    elif how=="S":    #2
        q_predict=self.vTable[tuple(location)]
            [self.actions.index(action)]    #3
        if isOver != True:    #4
            q_real=reward+
                self.lamda*
                self.vTable[tuple(newLocation)]
                [self.actions.index(self.locationNextMove)]    
        else:    #5
            q_real=reward
        self.vTable[tuple(location)]
            [self.actions.index(action)]+=
            self.lr*(q_real-q_predict)    #6
```
{% endcode %}

1. Q-Learning算法，这部分和我们前面提到的没有什么差别；
2. Sarsa算法
3. 查一下小本本，获取当前状态s做出行为a的估计值；
4. 按照公式\(1\)计算获取状态s时做出行为a获取到的实际价值；
5. 如果游戏结束，就不对未来折现了，因为已经没有未来了；
6. 按照公式\(2\)来更新小本本上状态s做出行为a的估计值。

与Q-Learning一样，为了避免陷入局部最优，Sarsa也会以一定的比例做出随机的行为。所不同的是，智能体如果使用Sarsa算法，在查表获取未来价值前会先确定好下一状态采取什么行为，也就是说使用Sarsa算法的智能体一次好久考虑两步行为。

| 格子 | 往右 | 往左 |
| :--- | :--- | :--- |
| 5 | 0.6 | 0.4 |
| 6 | 0.8 | 0.7 |

如果当前小人在第五格，智能体查了一下表，往右走的预估价值比较高，于是它选择了往右走的行动，但是为了按照Sarsa算法更新，它还要看一下在第六格时，它会采取什么行动，安理它同样应该选择继续往右走，但是由于随机到一个比较小的数字，它不得已随机选择了往左走。此时在更新第五格的行为表的时候，对未来第六格能获取价值的折现取的就是0.7，而不能是0.8了，而Q-Learning算法中，我们取的是MAX\(第六格\)，所以一定会取0.8。

{% code title="myGO/q-learning\_sarsa.py" %}
```python
action=self.locationNextMove    #1
beforeAction=self.location[:]    #2
isOver,reward=env.actionResp(action)
afterAction=env.getAgentLoc()[:]
self.location=afterAction[:]    #3
self.locationNextMove=self.chooseAction()    #4
self.dl(action,beforeAction,afterAction,how,env,isOver,reward)    #5
```
{% endcode %}

1. 获取当前状态下的行为a；
2. 获取当前状态s；
3. 获取s状态执行行为a后的下一状态s+1；
4. 获取s+1状态下的行动a’；
5. 使用Sarsa算法学习。

Q-Learning和Sarsa两者很难判定孰优孰劣。他们都是单步更新，算法也仅在更新估计值的细节上略有差异。在具体的问题中使用哪种方法读者可以根据喜好或者实验结果来决定。从定性的角度来说，Q-Learning更具侵略性，更贪婪一些，这使得它在决策树的选择上更倾向于深入挖掘。而Sarsa则更倾向于横向拓展。当学习到一个可行的策略后，Q-Learning就很难再有新的发现了，因为它在拓展能力上不足。Sarsa由于会随机走动，就还会能够发现新的东西，可能学到更好的策略。Q-Learning的收敛速度比Sarsa要快，而且一旦收敛后，每次学习后的结果偏差不大，而Sarsa的偏差会大很多，有时候我们根本不知道Sarsa是否已经找到了最优解。

### Sarsa的进化，Sarsa-Lamda

Q-Learning和Sarsa都是单步更新，而且两者在使用的效果上几乎没有差别。之所以要把Q-Learning修改成Sarsa的目的，是为了引出我们接着要解决的一个问题，单步更新的效率太低。

之前的算法中，如果学习没有深入到一定的程度，智能体每次开头的很长一段时间都只能不停地随机游走，这是因为我们Q-Learning的算法更新总是逆向的，而且每次还只能更新一个状态。如果完成游戏本身的最短路径很长，那么即使是最理想的更新过程也需要很久。但是在第一轮学习时获取价值以后，我们可以肯定一点，不管是正向的还是负面的价值，起码这条行为路径上的每个状态都和最后获取的价值或多或少的有一些关系。从这个角度出发，很自然的就可以想到，我们是不是可以在获取到价值后一次性更新整条行为路径上的状态估计值呢？前面提到，智能体在使用Q-Learning或者Sarsa算法的过程中一直在更新自己的预期值小本本，即使不能获得立即的价值回报，但是通过多轮学习，算法能够将未来的期望收益折现到当前的状态下来作为当前状态对未来行为的估计值。这个过程其实就是我们一次性批量更新价值的雏形，如果智能体能够记录下一轮学习过程中的行为，显然只要反向逆着状态记录做更新就可以一次性更新掉行为链上的所有值。

现在，智能体需要做好每一次行为的记录，然后每一次行为完成后就立即更新自己的小本本，同时把这个行为记录表上的行为也都按照算法更新一遍。原来智能体只需要一本小本本用来记录每个状态下不同行为的预估值，现在还需要多加一本小本本，作用是记录每一次行为的历史记录，这个小本本看上去是这样子的：

| 序号 | 状态 | 行为 |
| :--- | :--- | :--- |
| 1 | s1 | a1 |
| 2 | s2 | a2 |
| 3 | s3 | a3 |



其实也是对原始版的优化，在上面的描述，优化方法是随机，但是这有点作弊了，我们从上帝视角来为智能体创造了一些不同的”现实“，然后让智能体汇总起这些虚假”现实“中的经验。这让我想到了漫威和DC漫画中的各种时间线、各种平行宇宙的设定。lamda让更新一次完成一串。

为什么sasa可以lamda，q-Learning 不行，因为sasa是on-policy，另外一个是off，off因为不连贯，所以不能lamda

围棋神经网络的更新使用了Sarsa-lamda的思想。DQN其实叫DSLN（Deep Sarsa Lamda Network）更贴切一些。

