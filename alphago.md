# AlphaGo

在1997年5月11日之前，计算机想要在国际象棋上战胜人类还被认为是一件遥不可及的事情。深蓝第一次和卡斯帕罗夫对战是在1996年，当时深蓝以2比4落败。即便是在1997年深蓝击败卡斯帕罗夫之后，卡斯帕罗夫本人还是不愿意相信自己是被一台计算机击败的，他声称电脑进行了作弊，有人在背后帮助了电脑下棋。深蓝当时使用了专用的计算机芯片，那是为国际象棋游戏而专门设计的硬件设备，随着计算机技术的发展，现在任何一台计算机上都可以运行国际象棋程序并轻易击败大师级选手。即便计算机已经能够击败人类象棋选手，但是围棋一度被认为是计算机难以超越的。国际象棋的智能程序主要依靠α-β剪枝算法，再加上一些启发性下法和残局库，便可以达到大师级的棋力。围棋的智能程序也曾试图仿照国际象棋的方法，但是由于围棋的搜索广度和深度远远超过了国际象棋，仅仅依赖α-β剪枝目前还没有办法在有限的时间内完成搜索。国际象棋那一套方法在围棋上行不通，便有人尝试使用蒙特卡洛搜索树的方法来逼近围棋的最优解。 2008年，使用蒙特卡洛搜索树算法的MoGo软件在九路围棋中可以达到段位水平，同时Fuego程序可以在九路围棋中战胜实力强劲的业余棋手。2012年1月，Zen程序在19路围棋上以3：1击败二段棋手约翰·特朗普。但是这些成绩和深蓝当初设立下的里程碑还差的很远，特别是在19路围棋的表现上，计算机智能程序在职业选手面前的表现还是显得幼稚与低能。在AlphaGo出现之前，人们普遍认为能够战胜人类9段职业水平棋手的智能算法至少在10年内不会出现。在AlphaGo战胜李世石之前，已经有人在尝试使用流行的神经网络来提升围棋的棋力。由Facebook开发的Darkforest就是其中的佼佼者。Darkforest采用了深度卷积网络来提取围棋棋局的特征，并结合蒙特卡洛搜索树方法来进一步提升棋力。如果把Darkforest拿来和AlphaGo比较一下的话，明显的差异仅在于AlphaGo使用强化学习得来的策略网络要比直接通过监督学习得到的策略网络要高效的多的多。我们在前面介绍监督学习时候也提到，通过监督学习得到的策略网络仅仅学到了下围棋的形，由于学习样本中几乎很少有吃子的情况，网络在故意吃子的下法前表现的非常无知。

从蒙特卡洛搜索树的经验上来看，使用这种方法还是非常有潜力的，当时最大的问题主要是由于围棋的状态复杂度实在是太大，随机搜索有极大的概率将算能浪费在了显然无用的落子搜索上。虽然可以通过加入启发式的算法来解决这个问题，但是什么是好的启发算法呢，人工编辑的特征总是有限的，而且是主观的。AlphaGo主要是解决了如何提高蒙特卡洛搜索树有效搜索的问题。它通过引入可靠的策略网络来指导算法进行落子选择，依靠现代先进的硬件设备，这种方法克服了计算速度上的不足，能够有效地从统计的角度来提取围棋的各类特征。

我们人类选手在下任何棋的时候总是会尽可能地多考虑一些可选项，并且在一种选择下会尽可能的多思考几步。当思考的达到一定深度，脑子开始变得一片糊涂的时候，就凭感觉对思考的结果下一个判断。高手和初学者的区别仅在于对可选项的思考广度与深度不同，高手可能可以计算到10步以后的情况，而初学者可能仅仅只能判断3到4步后的棋局形势。高手在广度选取上也会与初学者不同，初学者可能只能够专注于某几个可选项，而大师级选手可能会计算到所有的可选项。

![&#x4EBA;&#x7C7B;&#x4E0B;&#x68CB;&#x65F6;&#x7684;&#x601D;&#x8003;&#x8FC7;&#x7A0B;](.gitbook/assets/ren-lei-.png)

人类具有直觉这项特殊能力，它使人类不需要经过复杂的过程计算就能对事件的结果给出一个八九不离十的判断。围棋中，当选手思考到一定的深度时就会凭直觉和经验对棋局的结果给出判断。初学者的直觉可能不是那么准确，但是职业选手总是能够信任自己的直觉，并且最终事实呈现的结果也与直觉相差不远。我们在之前的DQN网络中介绍的着法价值判断就有点类似于模拟人类的直觉。在AlphaGo算法里，我们再进一步增强这种计算机形式的直觉，使得智能体有能够拥有超越人类的“直觉”。

AlphaGo的下棋方法和人类棋手非常相似。它由三个深度卷积神经网络组成，分别是一个复杂的策略网络、一个简单的策略网络和一个价值评价网络。类比人类的

![](.gitbook/assets/alphago-mo-ni-ren-lei-.svg)

