---
description: >-
  AlphaGo是强化学习与蒙特卡洛搜索树相结合的产物，本章将详细介绍如何实现AlphaGo。本章将不会再的展示代码，程序上的实现方法在之前的章节都已经讲过。我想把精力都放在讲解上，读者可以尝试自己实现代码，或者查看源码。
---

# AlphaGo

在1997年5月11日之前，计算机想要在国际象棋上战胜人类还被认为是一件遥不可及的事情。深蓝第一次和卡斯帕罗夫对战是在1996年，当时深蓝以2比4落败。即便是在1997年深蓝击败卡斯帕罗夫之后，卡斯帕罗夫本人还是不愿意相信自己是被一台计算机击败的，他声称电脑进行了作弊，有人在背后帮助了电脑下棋。深蓝当时使用了专用的计算机芯片，那是为国际象棋游戏而专门设计的硬件设备，随着计算机技术的发展，现在任何一台计算机上都可以运行国际象棋程序并轻易击败大师级选手。即便计算机已经能够击败人类象棋选手，但是围棋一度被认为是计算机难以超越的。国际象棋的智能程序主要依靠α-β剪枝算法，再加上一些启发性下法和残局库，便可以达到大师级的棋力。围棋的智能程序也曾试图仿照国际象棋的方法，但是由于围棋的搜索广度和深度远远超过了国际象棋，仅仅依赖α-β剪枝目前还没有办法在有限的时间内完成搜索。国际象棋那一套方法在围棋上行不通，便有人尝试使用蒙特卡洛搜索树的方法来逼近围棋的最优解。 2008年，使用蒙特卡洛搜索树算法的MoGo软件在九路围棋中可以达到段位水平，同时Fuego程序可以在九路围棋中战胜实力强劲的业余棋手。2012年1月，Zen程序在19路围棋上以3：1击败二段棋手约翰·特朗普。但是这些成绩和深蓝当初设立下的里程碑还差的很远，特别是在19路围棋的表现上，计算机智能程序在职业选手面前的表现还是显得幼稚与低能。在AlphaGo出现之前，人们普遍认为能够战胜人类9段职业水平棋手的智能算法至少在10年内不会出现。在AlphaGo战胜李世石之前，已经有人在尝试使用流行的神经网络来提升围棋的棋力。由Facebook开发的Darkforest就是其中的佼佼者。Darkforest采用了深度卷积网络来提取围棋棋局的特征，并结合蒙特卡洛搜索树方法来进一步提升棋力。如果把Darkforest拿来和AlphaGo比较一下的话，明显的差异仅在于AlphaGo使用强化学习得来的策略网络要比直接通过监督学习得到的策略网络要高效的多的多。我们在前面介绍监督学习时候也提到，通过监督学习得到的策略网络仅仅学到了下围棋的形，由于学习样本中几乎很少有吃子的情况，网络在故意吃子的下法前表现的非常无知。

从蒙特卡洛搜索树的经验上来看，使用这种方法还是非常有潜力的，当时最大的问题主要是由于围棋的状态复杂度实在是太大，随机搜索有极大的概率将算能浪费在了显然无用的落子搜索上。虽然可以通过加入启发式的算法来解决这个问题，但是什么是好的启发算法呢，人工编辑的特征总是有限的，而且是主观的。AlphaGo主要是解决了如何提高蒙特卡洛搜索树有效搜索的问题。它通过引入可靠的策略网络来指导算法进行落子选择，依靠现代先进的硬件设备，这种方法克服了计算速度上的不足，能够有效地从统计的角度来提取围棋的各类特征。

我们人类选手在下任何棋的时候总是会尽可能地多考虑一些可选项，并且在一种选择下会尽可能的多思考几步。当思考的达到一定深度，脑子开始变得一片糊涂的时候，就凭感觉对思考的结果下一个判断。高手和初学者的区别仅在于对可选项的思考广度与深度不同，高手可能可以计算到10步以后的情况，而初学者可能仅仅只能判断3到4步后的棋局形势。高手在广度选取上也会与初学者不同，初学者可能只能够专注于某几个可选项，而大师级选手可能会计算到所有的可选项。

![&#x4EBA;&#x7C7B;&#x4E0B;&#x68CB;&#x65F6;&#x7684;&#x601D;&#x8003;&#x8FC7;&#x7A0B;](.gitbook/assets/ren-lei-.png)

人类具有直觉这项特殊能力，它使人类不需要经过复杂的过程计算就能对事件的结果给出一个八九不离十的判断。围棋中，当选手思考到一定的深度时就会凭直觉和经验对棋局的结果给出判断。初学者的直觉可能不是那么准确，但是职业选手总是能够信任自己的直觉，并且最终事实呈现的结果也与直觉相差不远。我们在之前的DQN网络中介绍的着法价值判断就有点类似于模拟人类的直觉。在AlphaGo算法里，我们再进一步增强这种计算机形式的直觉，使得智能体有能够拥有超越人类的“直觉”。

AlphaGo由三个深度卷积神经网络组成，分别是一个复杂的策略网络、一个简单的策略网络和一个价值评价网络。复杂策略网络会根据当前的棋局给出着法建议，但是和策略梯度选择着法的方式不同，AlphaGo并不仅仅依赖策略网络来进行着法的选择，它还需要结合价值网络来综合评估潜在的着法选择。这就像人类棋手一样，会综合评判各种可行的下法所带来的收益。当使用复杂策略网络计算到一定深度后，AlphaGo就不再使用价值网络和策略网络相结合的方式来进行着法选择，而是对高置信度的着法进行多轮蒙特卡洛仿真，并将仿真的结果用于回溯更新之前所有路径上的节点。这种下棋方法和人类棋手的思考过程非常相似。它利用复杂策略网络和价值网络对棋局的有效着法进行筛选，这样可以避免把计算能力浪费在许多无意义的着法上。AlphaGo通过复杂策略网络和价值网络相结合对棋局进行演算，就和人类棋手对棋盘落子进行选择和演算是一样的。人类大师级棋手能够演算深度达到10回合以后，AlphaGo对这个深度并没有限制，但是由于复杂网络规模庞大，在计算速度上没有什么优势，采用复杂网络来进行整个蒙特卡洛仿真过程是极其不经济的，而且也是没有必要的。如果复杂网络如果存在策略偏差，还会导致智能体下出错误的着法。因此，一般由复杂网络指导的演算深度在10层到20层左右，作为一个超参，读者可以自己选择适合自己计算机算能的深度，毕竟正式下围棋的每一步都有时间限制，不能把时间都花在复杂网络的计算上。为了计算速度的难题，AlphaGo采用简单策略网络用于指导蒙特卡洛仿真时的落子，简单网络结构相对简单，可以实现快速落子。利用简单网络进行蒙特卡洛仿真得到对落子后的形势判断就像人类选手对棋局的演算达到了自己的极限，剩下的依靠直觉是一样的。目前的科学技术还没有办法解决直觉这个东西，计算机也没有直觉这个概念，我们用蒙特卡洛方法来多次模拟棋局，以此期望对后续形势有一个准确的判断，这就和直觉达到的效果是一样的，甚至比人类的直觉更准确。

![](.gitbook/assets/alphago-xia-qi-si-lu-.svg)

对比Alphago和之前我们使用梯度策略的方式来下棋，显然从alphago的角度来看，显然仅仅用神经网络来拟合围棋是很困难的一件事情。可能我们无法找到一个合适的网络，也可能这个网络相当庞大，目前的技术能力无法在有限的时间内完成训练和学习。但是，无论怎么说，使用蒙特卡洛方法来逼近围棋函数是不得已才为之的。另外使用简单网络而不是使用复杂网络来做仿真模拟，不是因为简单网络比复杂网络运行速度快，因为运行速度这件事情可以通过并行计算解决，而且alphago每一步才仿真1600局对弈，用1600台计算机做并行计算对于谷歌来说并不是一件什么大不了的事情。使用简单网络的目的主要是为了避免仿真时由于复杂网络在策略上的傲慢，从而导致结果产生偏差。简单网络仅仅是为了避免下一些无意义的落子而已，如果计算能力足够，使用随机落子法也是可以的，但是显而易见，随机落子的效率要差一点，现在来看，alphago选择使用简单策略，应该也从侧面反应了简单策略会比随机策略要高效。说这些的目的，是为了提醒读者，没有必要对简单网络做过多的训练或者让它学习复杂的内容。另外，在用简单网络做仿真模拟时，可以以一定概率引入随机策略，目的也是为了进一步降低由于网络的局部最优特性而引起的结果偏差。

AlphaGo使用3个网络，两个策略网络，一个简单，一个复杂，复杂的用于给出可信度高的着法，简答的用于蒙特卡罗仿真，一个价值网络。我们可以使用ac网络来训练策略网络和价值网络，也可以单独训练3个网络。考虑到单独训练3个网络可以更好地利用并发机器资源，这里选择单独训练的方法。读者如果想提高训练的效率可以使用ac网络，具体的实现方法前面已经有介绍。对于网络训练方面，这一章没有更新的内容，我们基本上是复制前面介绍的梯度策略网络，价值网络或者ac网络。我们需要将注意力集中在alphago的实现方法。

